{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635f1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# feature selection libraries\n",
    "from boruta import BorutaPy\n",
    "from scipy.stats import normaltest\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#### others\n",
    "# additional, outlier detection?\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# maybe in use - pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3fd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa_train = pd.read_csv(\"data/artificial_train.data\", sep = \" \", header = None).loc[:, 0:499]\n",
    "ya_train = pd.read_csv(\"data/artificial_train.labels\", sep = \" \", header = None)\n",
    "Xa_test = pd.read_csv(\"data/artificial_valid.data\", sep = \" \", header = None).loc[:, 0:499]\n",
    "\n",
    "Xd_train = pd.read_csv(\"data/digits_train.data\", sep = \" \", header = None).loc[:, 0:4999]\n",
    "yd_train = pd.read_csv(\"data/digits_train.labels\", sep = \" \", header = None)\n",
    "Xd_test = pd.read_csv(\"data/digits_valid.data\", sep = \" \", header = None).loc[:, 0:4999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c09eb",
   "metadata": {},
   "source": [
    "# Variance Threshold \n",
    "Removing columns with 0 variance in test or train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4306306e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 111,  119,  196,  421,  479,  793,  876, 1037, 1040, 1179, 1267,\n",
       "        1639, 1736, 1792, 1835, 1904, 2022, 2086, 2198, 2248, 2348, 2585,\n",
       "        2604, 2686, 2691, 2811, 2901, 2909, 2952, 3007, 3026, 3157, 3193,\n",
       "        3476, 3556, 3631, 3706, 3745, 3864, 4066, 4100, 4255, 4388, 4872,\n",
       "        4964], dtype=int64),)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(Xd_train.var()== 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eb850d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  18,   21,   29,   50,   63,  107,  119,  129,  137,  149,  157,\n",
       "         175,  182,  242,  299,  339,  350,  421,  437,  460,  475,  479,\n",
       "         484,  539,  565,  608,  620,  625,  628,  635,  647,  650,  664,\n",
       "         674,  675,  705,  713,  737,  738,  744,  773,  775,  790,  793,\n",
       "         809,  822,  850,  876,  899,  901,  913,  924,  945,  971,  972,\n",
       "         975,  983,  994, 1004, 1010, 1076, 1107, 1117, 1123, 1136, 1140,\n",
       "        1141, 1146, 1148, 1167, 1169, 1242, 1254, 1261, 1284, 1290, 1329,\n",
       "        1397, 1412, 1434, 1455, 1483, 1489, 1505, 1510, 1514, 1527, 1538,\n",
       "        1627, 1639, 1644, 1676, 1681, 1704, 1711, 1726, 1747, 1749, 1752,\n",
       "        1774, 1792, 1797, 1808, 1825, 1827, 1832, 1835, 1888, 1994, 2000,\n",
       "        2011, 2020, 2022, 2033, 2055, 2059, 2066, 2070, 2085, 2089, 2092,\n",
       "        2093, 2097, 2108, 2115, 2166, 2182, 2193, 2198, 2204, 2216, 2236,\n",
       "        2248, 2255, 2261, 2276, 2339, 2348, 2357, 2364, 2371, 2380, 2389,\n",
       "        2396, 2423, 2425, 2428, 2432, 2435, 2441, 2444, 2453, 2454, 2459,\n",
       "        2462, 2463, 2477, 2506, 2529, 2531, 2548, 2578, 2580, 2585, 2593,\n",
       "        2602, 2604, 2610, 2654, 2683, 2685, 2686, 2703, 2706, 2716, 2718,\n",
       "        2723, 2737, 2747, 2776, 2785, 2811, 2816, 2824, 2826, 2844, 2875,\n",
       "        2901, 2913, 2932, 2939, 2949, 2967, 3007, 3014, 3018, 3024, 3026,\n",
       "        3071, 3080, 3090, 3163, 3174, 3193, 3194, 3199, 3210, 3228, 3250,\n",
       "        3261, 3267, 3274, 3277, 3343, 3346, 3381, 3462, 3474, 3476, 3536,\n",
       "        3541, 3556, 3581, 3591, 3621, 3629, 3631, 3706, 3763, 3810, 3839,\n",
       "        3842, 3854, 3864, 3904, 3933, 3972, 3985, 4051, 4066, 4072, 4100,\n",
       "        4142, 4149, 4159, 4212, 4236, 4246, 4255, 4284, 4285, 4396, 4399,\n",
       "        4418, 4462, 4481, 4508, 4509, 4513, 4515, 4521, 4535, 4568, 4582,\n",
       "        4600, 4601, 4602, 4606, 4609, 4626, 4634, 4650, 4658, 4670, 4676,\n",
       "        4715, 4760, 4765, 4775, 4792, 4803, 4871, 4937, 4950, 4964, 4969],\n",
       "       dtype=int64),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(Xd_test.var()== 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80929c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where(Xd_train.var()== 0)\n",
    "b = np.where(Xd_test.var()== 0)\n",
    "x = np.append(a, b)\n",
    "x = np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b491277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_train = Xd_train.drop(x, axis = 1)\n",
    "Xd_test = Xd_test.drop(x, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8a8a4",
   "metadata": {},
   "source": [
    "# Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f70746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler(with_std = False)\n",
    "scaler2 = StandardScaler(with_std = False)\n",
    "\n",
    "scaler1.fit(Xa_train)\n",
    "scaler2.fit(Xd_train)\n",
    "\n",
    "columns1 = Xa_train.columns\n",
    "columns2 = Xd_train.columns\n",
    "\n",
    "Xa_train = pd.DataFrame(scaler1.transform(Xa_train))\n",
    "Xa_test = pd.DataFrame(scaler1.transform(Xa_test))\n",
    "\n",
    "Xd_train = pd.DataFrame(scaler2.transform(Xd_train))\n",
    "Xd_test = pd.DataFrame(scaler2.transform(Xd_test))\n",
    "\n",
    "Xa_train.columns = columns1\n",
    "Xa_test.columns = columns1\n",
    "\n",
    "Xd_train.columns = columns2\n",
    "Xd_test.columns = columns2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e70d9",
   "metadata": {},
   "source": [
    "# Feature selection methods - implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "0b47af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed up to 2*k features having lowest variance (union of 'up to k from train' and 'up to k from test')\n",
    "def rm2KofLowestVariance(data_train, data_test, k):\n",
    "    variances = data_train.var()\n",
    "    sorted_variances = np.sort(variances)\n",
    "    if(len(sorted_variances) < k + 1):\n",
    "        return\n",
    "    threshold = sorted_variances[k]\n",
    "    aux = data_train.columns[np.where(variances < threshold)]\n",
    "\n",
    "    variances = data_test.var()\n",
    "    sorted_variances = np.sort(variances)\n",
    "    if(len(sorted_variances) < k + 1):\n",
    "        return\n",
    "    threshold = sorted_variances[k]\n",
    "    aux2 = data_test.columns[np.where(variances < threshold)]\n",
    "\n",
    "    x = np.append(aux, aux2)\n",
    "    x = np.unique(x)\n",
    "\n",
    "    return data_train.drop(x,axis = 1), data_test.drop(x,axis = 1)\n",
    "\n",
    "# removed up to 2*k features having lowest p_value of normality test (union of 'up to k from train' and 'up to k from test')\n",
    "def rm2KofFeaturesFromNormalDistribution(data_train, data_test, k):\n",
    "    tt = normaltest(data_train)\n",
    "    p_values = tt[1]\n",
    "    sorted_p = np.sort(p_values)\n",
    "    if(len(sorted_p) < k + 1):\n",
    "        return\n",
    "    threshold = sorted_p[k]\n",
    "    aux = data_train.columns[np.where(p_values < threshold)]\n",
    "\n",
    "    tt = normaltest(data_test)\n",
    "    p_values = tt[1]\n",
    "    sorted_p = np.sort(p_values)\n",
    "    if(len(sorted_p) < k + 1):\n",
    "        return\n",
    "    threshold = sorted_p[k]\n",
    "    aux2 = data_test.columns[np.where(p_values < threshold)]\n",
    "\n",
    "    x = np.append(aux, aux2)\n",
    "    x = np.unique(x)\n",
    "\n",
    "    return data_train.drop(x,axis = 1), data_test.drop(x,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "7196e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyBorutaSelection(data_train, labels, data_test, classifModel):\n",
    "    # define Boruta feature selection method\n",
    "    feat_selector = BorutaPy(classifModel, n_estimators='auto', random_state=110)\n",
    "\n",
    "    # find all relevant features - 5 features should be selected\n",
    "    feat_selector.fit(data_train.values, labels.values.ravel())\n",
    "\n",
    "    return data_train.iloc[:,feat_selector.support_], data_test.iloc[:,feat_selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ae7faadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaveKusingVIF(data_train, data_test, k):\n",
    "    while k < len(data_train.columns):\n",
    "        ds=pd.Series([variance_inflation_factor(data_train.values, i)   \n",
    "           for i in range(data_train.shape[1])],   \n",
    "              index=data_train.columns)  \n",
    "        ds2=pd.Series([variance_inflation_factor(data_test.values, i)   \n",
    "               for i in range(data_test.shape[1])],   \n",
    "              index=data_test.columns)\n",
    "\n",
    "        if ds.values[np.argmax(ds)] >= ds2.values[np.argmax(ds2)]:\n",
    "            col_to_be_deleted = data_train.columns[np.argmax(ds)]\n",
    "        else:\n",
    "            col_to_be_deleted = data_train.columns[np.argmax(ds2)]\n",
    "        data_train = data_train.drop(col_to_be_deleted,axis=1)\n",
    "        data_test = data_test.drop(col_to_be_deleted,axis=1)\n",
    "    return data_train, data_test\n",
    "\n",
    "def leaveKusingVIF_QUICK(data_train, data_test, k):\n",
    "    while k < len(data_train.columns):\n",
    "        ds=pd.Series([variance_inflation_factor(data_train.values, i)   \n",
    "           for i in range(data_train.shape[1])],   \n",
    "              index=data_train.columns)  \n",
    "        col_to_be_deleted = data_train.columns[np.argmax(ds)]\n",
    "\n",
    "        data_train = data_train.drop(col_to_be_deleted,axis=1)\n",
    "        data_test = data_test.drop(col_to_be_deleted,axis=1)\n",
    "    return data_train, data_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bf1e2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1, dd2 = leaveKusingVIF(df1, df2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2b23c6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>48</th>\n",
       "      <th>204</th>\n",
       "      <th>318</th>\n",
       "      <th>336</th>\n",
       "      <th>433</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.0005</td>\n",
       "      <td>-4.9565</td>\n",
       "      <td>-72.4</td>\n",
       "      <td>177.126</td>\n",
       "      <td>34.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.9995</td>\n",
       "      <td>6.0435</td>\n",
       "      <td>-17.4</td>\n",
       "      <td>-11.874</td>\n",
       "      <td>-56.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-25.0005</td>\n",
       "      <td>-16.9565</td>\n",
       "      <td>19.6</td>\n",
       "      <td>-15.874</td>\n",
       "      <td>70.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.9995</td>\n",
       "      <td>-24.9565</td>\n",
       "      <td>-28.4</td>\n",
       "      <td>-82.874</td>\n",
       "      <td>66.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-56.0005</td>\n",
       "      <td>19.0435</td>\n",
       "      <td>-24.4</td>\n",
       "      <td>-95.874</td>\n",
       "      <td>-36.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>19.9995</td>\n",
       "      <td>39.0435</td>\n",
       "      <td>-65.4</td>\n",
       "      <td>-17.874</td>\n",
       "      <td>104.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-37.0005</td>\n",
       "      <td>18.0435</td>\n",
       "      <td>57.6</td>\n",
       "      <td>-238.874</td>\n",
       "      <td>-23.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>58.9995</td>\n",
       "      <td>34.0435</td>\n",
       "      <td>57.6</td>\n",
       "      <td>57.126</td>\n",
       "      <td>72.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>4.9995</td>\n",
       "      <td>21.0435</td>\n",
       "      <td>-26.4</td>\n",
       "      <td>67.126</td>\n",
       "      <td>109.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-54.0005</td>\n",
       "      <td>53.0435</td>\n",
       "      <td>8.6</td>\n",
       "      <td>-172.874</td>\n",
       "      <td>-45.727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          48       204   318      336      433\n",
       "0    -45.0005  -4.9565 -72.4  177.126   34.273\n",
       "1     13.9995   6.0435 -17.4  -11.874  -56.727\n",
       "2    -25.0005 -16.9565  19.6  -15.874   70.273\n",
       "3     43.9995 -24.9565 -28.4  -82.874   66.273\n",
       "4    -56.0005  19.0435 -24.4  -95.874  -36.727\n",
       "...       ...      ...   ...      ...      ...\n",
       "1995  19.9995  39.0435 -65.4  -17.874  104.273\n",
       "1996 -37.0005  18.0435  57.6 -238.874  -23.727\n",
       "1997  58.9995  34.0435  57.6   57.126   72.273\n",
       "1998   4.9995  21.0435 -26.4   67.126  109.273\n",
       "1999 -54.0005  53.0435   8.6 -172.874  -45.727\n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2eaeb1",
   "metadata": {},
   "source": [
    "# Defining Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a7db935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# m - number of features in use\n",
    "def score1(y_pred, y_true, m):\n",
    "    BA = balanced_accuracy_score(y_true, y_pred) # changement of order\n",
    "    if m > 5:\n",
    "        BA = BA - 0.01*(m/5 - 1)\n",
    "    return BA\n",
    "def score2(y_pred, y_true, m):\n",
    "    BA = balanced_accuracy_score(y_true, y_pred) # changement of order\n",
    "    if m > 50:\n",
    "        BA = BA - 0.01*(m/200 - 0.25)\n",
    "    return BA\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebec120e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.unique([1,1,1,1,1,-1,-1,-1], return_counts = True)[0] == [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc940be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76f94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "607a6183",
   "metadata": {},
   "source": [
    "# Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeated\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SEEDS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da9fcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 50, random_state = 110, n_jobs= -1, max_depth = 5)\n",
    "xg = GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 50, max_depth = 5, random_state = 110)\n",
    "lr = LogisticRegression(C = 1.0, random_state = 110)\n",
    "svc = CalibratedClassifierCV(LinearSVC(random_state = 110, C = 1.0, max_iter= 1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "44f42911",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"model1_rf\": RandomForestClassifier(n_estimators = 20, random_state = 110, n_jobs= -1, max_depth = 4),\n",
    "    \"model2_rf\": RandomForestClassifier(n_estimators = 50, random_state = 110, n_jobs= -1, max_depth = 5),\n",
    "    \"model3_rf\": RandomForestClassifier(n_estimators = 80, random_state = 110, n_jobs= -1, max_depth = 6),\n",
    "    \"model1_xg\": GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 20, max_depth = 4, random_state = 110),\n",
    "    \"model2_xg\": GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 50, max_depth = 5, random_state = 110),\n",
    "    \"model3_xg\": GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 80, max_depth = 6, random_state = 110),\n",
    "    \"model4_xg\": GradientBoostingClassifier(learning_rate = 0.01, n_estimators = 50, max_depth = 5, random_state = 110),\n",
    "    \"model5_xg\": GradientBoostingClassifier(learning_rate = 0.3, n_estimators = 50, max_depth = 5, random_state = 110),\n",
    "    \"model1_lr\": LogisticRegression(C = 0.4, random_state = 110),\n",
    "    \"model2_lr\": LogisticRegression(C = 1.0, random_state = 110),\n",
    "    \"model3_lr\": LogisticRegression(C = 2.5, random_state = 110),\n",
    "    \"model1_svc\": CalibratedClassifierCV(LinearSVC(random_state = 110, C = 0.4, max_iter= 1000)),\n",
    "    \"model2_svc\": CalibratedClassifierCV(LinearSVC(random_state = 110, C = 1.0, max_iter= 1000)),\n",
    "    \"model3_svc\": CalibratedClassifierCV(LinearSVC(random_state = 110, C = 2.5, max_iter= 1000)),\n",
    "    \"model1_ensemble_rfxglr\": VotingClassifier([(\"m1\",RandomForestClassifier(n_estimators = 80, random_state = 110, n_jobs= -1, max_depth = 5)),\n",
    "                                         (\"m2\",GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 50, max_depth = 5, random_state = 110)),\n",
    "                                               (\"m3\", LogisticRegression(C = 1.0, random_state = 110))], \n",
    "                                        voting='soft'),\n",
    "    \"model2_ensemble_rflrsvc\": VotingClassifier([(\"m1\",RandomForestClassifier(n_estimators = 80, random_state = 110, n_jobs= -1, max_depth = 6)),\n",
    "                                         (\"m2\",CalibratedClassifierCV(LinearSVC(random_state = 110, C = 1.0, max_iter= 1000))),\n",
    "                                               (\"m3\",LogisticRegression(C = 1.0, random_state = 110))], \n",
    "                                        voting='soft'),\n",
    "    \"model3_ensemble_xglrsvc\": VotingClassifier([(\"m1\",GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 80, max_depth = 6, random_state = 110)),\n",
    "                                         (\"m2\",CalibratedClassifierCV(LinearSVC(random_state = 110, C = 1.0, max_iter= 1000))),\n",
    "                                               (\"m3\",LogisticRegression(C = 1.0, random_state = 110))], \n",
    "                                        voting='soft'),\n",
    "    \"model4_ensemble_rfxgsvc\": VotingClassifier([(\"m1\",GradientBoostingClassifier(learning_rate = 0.1, n_estimators = 100, max_depth = 5, random_state = 110)),\n",
    "                                         (\"m2\",CalibratedClassifierCV(LinearSVC(random_state = 110, C = 1.0, max_iter= 1000))),\n",
    "                                               (\"m3\",RandomForestClassifier(n_estimators = 100, random_state = 110, n_jobs= -1, max_depth = 5))], \n",
    "                                        voting='soft')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9dadd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variance_ks = [0,5,15,25,35,50,100]\n",
    "normality_ks = [0,5,15,25,35,50,100]\n",
    "boruta_flags = [True] # always use boruta\n",
    "vif_ks = [5,6,7,8,9,10,12,15]\n",
    "\n",
    "feature_selection_cases = np.array(np.meshgrid(variance_ks, normality_ks, boruta_flags, vif_ks)).T.reshape(-1,4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ad40e01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 4)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection_cases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44cded",
   "metadata": {},
   "source": [
    "# Train/test split - 75%/25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dffe87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_val, y1_train, y1_val =  train_test_split(Xa_train, ya_train,train_size = 0.75, random_state = 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9009481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 500)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24f0c048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e24fe00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2c253bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bedf4",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf7ae97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"model_name\": {},\"variance_k\":{}, \"normality_k\":{}, \"boruta_flag\": {}, \"vif_k\":{}, \"score\": {}, \"chosen_columns\":{}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29327859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>variance_k</th>\n",
       "      <th>normality_k</th>\n",
       "      <th>boruta_flag</th>\n",
       "      <th>vif_k</th>\n",
       "      <th>score</th>\n",
       "      <th>chosen_columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name  variance_k  normality_k  boruta_flag  vif_k  score  \\\n",
       "0         0.0         0.0          0.0          0.0    0.0    0.0   \n",
       "\n",
       "   chosen_columns  \n",
       "0             0.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.append({\"model_name\": 0, \"variance_k\": 0, \"normality_k\": 0, \"boruta_flag\": 0, \"vif_k\":0, \"score\": 0, \"chosen_columns\": 0}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d2b8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({\"model_name\": {},\"variance_k\":{}, \"normality_k\":{}, \"boruta_flag\": {}, \"vif_k\":{}, \"score\": {}, \"chosen_columns\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e8bc5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE: 1/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 2/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 3/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 4/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 5/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 6/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 7/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 8/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 9/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 10/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 11/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 12/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 13/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 14/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 15/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 16/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 17/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 18/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 19/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 20/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 21/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 22/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 23/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 24/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 25/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 26/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 27/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 28/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 29/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 30/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 31/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 32/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 33/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 34/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 35/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 36/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 37/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 38/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 39/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 40/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 41/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 42/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 43/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 44/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 45/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 46/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 47/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 48/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 49/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 50/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 51/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 52/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 53/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 54/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 55/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 56/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 57/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 58/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 59/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 60/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 61/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 62/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 63/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 64/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 65/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 66/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 67/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 68/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 69/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 70/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 71/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 72/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 73/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 74/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 75/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 76/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 77/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 78/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 79/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 80/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 81/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 82/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 83/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 84/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 85/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 86/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 87/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 88/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 89/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 90/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 91/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 92/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 93/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 94/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 95/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 96/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 97/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 98/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 99/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 100/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 101/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 102/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 103/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 104/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 105/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 106/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 107/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 108/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 109/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 110/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 111/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 112/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 113/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 114/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 115/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 116/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 117/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 118/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 119/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 120/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 121/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 122/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 123/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 124/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 125/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 126/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 127/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 128/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 129/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 130/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 131/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 132/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 133/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 134/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 135/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 136/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 137/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 138/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 139/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 140/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 141/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 142/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 143/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 144/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 145/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 146/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 147/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 148/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 149/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 150/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 151/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 152/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 153/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 154/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 155/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 156/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 157/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 158/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 159/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 160/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 161/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 162/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 163/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 164/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 165/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 166/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 167/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 168/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 169/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 170/392\n",
      "Feature selection finished! m =  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE: 171/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 172/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 173/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 174/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 175/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 176/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 177/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 178/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 179/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 180/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 181/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 182/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 183/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 184/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 185/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 186/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 187/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 188/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 189/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 190/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 191/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 192/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 193/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 194/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 195/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 196/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 197/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 198/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 199/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 200/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 201/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 202/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 203/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 204/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 205/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 206/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 207/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 208/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 209/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 210/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 211/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 212/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 213/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 214/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 215/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 216/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 217/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 218/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 219/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 220/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 221/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 222/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 223/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 224/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 225/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 226/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 227/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 228/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 229/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 230/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 231/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 232/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 233/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 234/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 235/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 236/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 237/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 238/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 239/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 240/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 241/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 242/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 243/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 244/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 245/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 246/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 247/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 248/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 249/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 250/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 251/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 252/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 253/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 254/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 255/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 256/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 257/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 258/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 259/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 260/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 261/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 262/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 263/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 264/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 265/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 266/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 267/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 268/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 269/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 270/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 271/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 272/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 273/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 274/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 275/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 276/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 277/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 278/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 279/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 280/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 281/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 282/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 283/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 284/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 285/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 286/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 287/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 288/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 289/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 290/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 291/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 292/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 293/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 294/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 295/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 296/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 297/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 298/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 299/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 300/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 301/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 302/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 303/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 304/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 305/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 306/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 307/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 308/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 309/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 310/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 311/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 312/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 313/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 314/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 315/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 316/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 317/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 318/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 319/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 320/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 321/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 322/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 323/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 324/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 325/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 326/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 327/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 328/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 329/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 330/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 331/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 332/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 333/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 334/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 335/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 336/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 337/392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection finished! m =  12\n",
      "CASE: 338/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 339/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 340/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 341/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 342/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 343/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 344/392\n",
      "Feature selection finished! m =  15\n",
      "CASE: 345/392\n",
      "Feature selection finished! m =  13\n",
      "CASE: 346/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 347/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 348/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 349/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 350/392\n",
      "Feature selection finished! m =  4\n",
      "CASE: 351/392\n",
      "Feature selection finished! m =  15\n",
      "CASE: 352/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 353/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 354/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 355/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 356/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 357/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 358/392\n",
      "Feature selection finished! m =  15\n",
      "CASE: 359/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 360/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 361/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 362/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 363/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 364/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 365/392\n",
      "Feature selection finished! m =  15\n",
      "CASE: 366/392\n",
      "Feature selection finished! m =  13\n",
      "CASE: 367/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 368/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 369/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 370/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 371/392\n",
      "Feature selection finished! m =  6\n",
      "CASE: 372/392\n",
      "Feature selection finished! m =  15\n",
      "CASE: 373/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 374/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 375/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 376/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 377/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 378/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 379/392\n",
      "Feature selection finished! m =  15\n",
      "CASE: 380/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 381/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 382/392\n",
      "Feature selection finished! m =  10\n",
      "CASE: 383/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 384/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 385/392\n",
      "Feature selection finished! m =  5\n",
      "CASE: 386/392\n",
      "Feature selection finished! m =  15\n",
      "CASE: 387/392\n",
      "Feature selection finished! m =  12\n",
      "CASE: 388/392\n",
      "Feature selection finished! m =  11\n",
      "CASE: 389/392\n",
      "Feature selection finished! m =  9\n",
      "CASE: 390/392\n",
      "Feature selection finished! m =  8\n",
      "CASE: 391/392\n",
      "Feature selection finished! m =  7\n",
      "CASE: 392/392\n",
      "Feature selection finished! m =  4\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(110)\n",
    "\n",
    "data_train = X1_train\n",
    "data_val = X1_val\n",
    "\n",
    "# initial model for boruta\n",
    "boruta_model = RandomForestClassifier(n_estimators = 50, random_state = 110, n_jobs= -1, max_depth = 5)\n",
    "\n",
    "counter = 1\n",
    "res = pd.DataFrame({\"model_name\": {},\"variance_k\":{}, \"normality_k\":{}, \"boruta_flag\": {}, \"vif_k\":{}, \"score\": {}, \"chosen_columns\": {}})\n",
    "\n",
    "\n",
    "for case in feature_selection_cases:\n",
    "    print(\"CASE: \" + str(counter) + \"/392\")\n",
    "    counter += 1\n",
    "    \n",
    "    # new data for each feature selection case:\n",
    "    data_train = X1_train\n",
    "    data_val = X1_val\n",
    "    \n",
    "    # FEATURE SELECTION METHODS:\n",
    "    data_train, data_val = rm2KofLowestVariance(data_train, data_val, case[0])\n",
    "    data_train, data_val = rm2KofFeaturesFromNormalDistribution(data_train, data_val, case[1])\n",
    "    if case[2]:\n",
    "        data_train, data_val = applyBorutaSelection(data_train, y1_train, data_val, boruta_model)\n",
    "    data_train, data_val = leaveKusingVIF(data_train, data_val, case[3])\n",
    "    \n",
    "    chosen_columns = str(data_train.columns.values)[1:-1]\n",
    "    \n",
    "    m = data_train.shape[1]\n",
    "    \n",
    "    # STANDARDIZING\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler1.fit(data_train)\n",
    "    columns1 = data_train.columns\n",
    "    data_train = pd.DataFrame(scaler1.transform(data_train))\n",
    "    data_val = pd.DataFrame(scaler1.transform(data_val))\n",
    "    data_train.columns = columns1\n",
    "    data_val.columns = columns1    \n",
    "    \n",
    "    # ML learning\n",
    "    print(\"Feature selection finished! m = \", m)\n",
    "    \n",
    "    for key, model in models_dict.items():\n",
    "        model.fit(data_train, y1_train)\n",
    "        predictions = model.predict(data_val)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res = res.append({\"model_name\": key, \"variance_k\": case[0], \"normality_k\": case[1], \"boruta_flag\": case[2], \"vif_k\":case[3], \"score\": score, \"chosen_columns\": chosen_columns}, ignore_index = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "116de5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>variance_k</th>\n",
       "      <th>normality_k</th>\n",
       "      <th>boruta_flag</th>\n",
       "      <th>vif_k</th>\n",
       "      <th>score</th>\n",
       "      <th>chosen_columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5171</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.847848</td>\n",
       "      <td>10  64 105 204 241 281 318 378 453 481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>10  48  64 204 241 281 318 378 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>10  48  64 204 241 281 318 378 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>10  48  64 204 241 281 318 378 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>10  48  64 204 241 281 318 378 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>10  48  64 204 241 281 318 378 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>10  48  64 204 241 281 318 378 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.842870</td>\n",
       "      <td>10  28  48  64 105 204 338 442 453 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3155</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.840527</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.840527</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.840527</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.840527</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.840527</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.840527</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.839210</td>\n",
       "      <td>10  28  48  64 105 204 338 442 453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.838188</td>\n",
       "      <td>10  48  64 105 204 241 281 318 378 442 453 481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3157</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3283</th>\n",
       "      <td>model5_xg</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>10  48  64 204 241 281 318 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6935</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.835279</td>\n",
       "      <td>10  28  48  64 105 153 204 241 281 318 378 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.835122</td>\n",
       "      <td>10  48  64 204 241 281 318 378 451 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.835122</td>\n",
       "      <td>10  48  64 204 241 281 318 378 451 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.835122</td>\n",
       "      <td>10  48  64 204 241 281 318 378 451 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.835122</td>\n",
       "      <td>10  48  64 204 241 281 318 378 451 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.835122</td>\n",
       "      <td>10  48  64 204 241 281 318 378 451 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>model3_xg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.835122</td>\n",
       "      <td>10  48  64 204 241 281 318 378 451 493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>model2_xg</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.834739</td>\n",
       "      <td>10  48  64 204 281 318 493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name  variance_k  normality_k  boruta_flag  vif_k     score  \\\n",
       "5171  model3_xg       100.0          0.0          1.0   10.0  0.847848   \n",
       "3533  model3_xg         0.0          0.0          1.0    9.0  0.845151   \n",
       "4163  model3_xg        50.0          0.0          1.0    9.0  0.845151   \n",
       "3911  model3_xg        25.0          0.0          1.0    9.0  0.845151   \n",
       "3785  model3_xg        15.0          0.0          1.0    9.0  0.845151   \n",
       "3659  model3_xg         5.0          0.0          1.0    9.0  0.845151   \n",
       "4037  model3_xg        35.0          0.0          1.0    9.0  0.845151   \n",
       "4813  model5_xg        25.0          5.0          1.0   10.0  0.842870   \n",
       "3155  model3_xg        35.0          0.0          1.0    8.0  0.840527   \n",
       "3281  model3_xg        50.0          0.0          1.0    8.0  0.840527   \n",
       "2777  model3_xg         5.0          0.0          1.0    8.0  0.840527   \n",
       "2903  model3_xg        15.0          0.0          1.0    8.0  0.840527   \n",
       "2651  model3_xg         0.0          0.0          1.0    8.0  0.840527   \n",
       "3029  model3_xg        25.0          0.0          1.0    8.0  0.840527   \n",
       "3931  model5_xg        25.0          5.0          1.0    9.0  0.839210   \n",
       "6053  model3_xg       100.0          0.0          1.0   12.0  0.838188   \n",
       "3031  model5_xg        25.0          0.0          1.0    8.0  0.836272   \n",
       "2779  model5_xg         5.0          0.0          1.0    8.0  0.836272   \n",
       "2653  model5_xg         0.0          0.0          1.0    8.0  0.836272   \n",
       "3157  model5_xg        35.0          0.0          1.0    8.0  0.836272   \n",
       "2905  model5_xg        15.0          0.0          1.0    8.0  0.836272   \n",
       "3283  model5_xg        50.0          0.0          1.0    8.0  0.836272   \n",
       "6935  model3_xg       100.0          0.0          1.0   15.0  0.835279   \n",
       "4919  model3_xg        35.0          0.0          1.0   10.0  0.835122   \n",
       "4541  model3_xg         5.0          0.0          1.0   10.0  0.835122   \n",
       "4793  model3_xg        25.0          0.0          1.0   10.0  0.835122   \n",
       "5045  model3_xg        50.0          0.0          1.0   10.0  0.835122   \n",
       "4667  model3_xg        15.0          0.0          1.0   10.0  0.835122   \n",
       "4415  model3_xg         0.0          0.0          1.0   10.0  0.835122   \n",
       "2398  model2_xg        50.0          0.0          1.0    7.0  0.834739   \n",
       "\n",
       "                                         chosen_columns  \n",
       "5171             10  64 105 204 241 281 318 378 453 481  \n",
       "3533                 10  48  64 204 241 281 318 378 493  \n",
       "4163                 10  48  64 204 241 281 318 378 493  \n",
       "3911                 10  48  64 204 241 281 318 378 493  \n",
       "3785                 10  48  64 204 241 281 318 378 493  \n",
       "3659                 10  48  64 204 241 281 318 378 493  \n",
       "4037                 10  48  64 204 241 281 318 378 493  \n",
       "4813             10  28  48  64 105 204 338 442 453 493  \n",
       "3155                     10  48  64 204 241 281 318 493  \n",
       "3281                     10  48  64 204 241 281 318 493  \n",
       "2777                     10  48  64 204 241 281 318 493  \n",
       "2903                     10  48  64 204 241 281 318 493  \n",
       "2651                     10  48  64 204 241 281 318 493  \n",
       "3029                     10  48  64 204 241 281 318 493  \n",
       "3931                 10  28  48  64 105 204 338 442 453  \n",
       "6053     10  48  64 105 204 241 281 318 378 442 453 481  \n",
       "3031                     10  48  64 204 241 281 318 493  \n",
       "2779                     10  48  64 204 241 281 318 493  \n",
       "2653                     10  48  64 204 241 281 318 493  \n",
       "3157                     10  48  64 204 241 281 318 493  \n",
       "2905                     10  48  64 204 241 281 318 493  \n",
       "3283                     10  48  64 204 241 281 318 493  \n",
       "6935   10  28  48  64 105 153 204 241 281 318 378 43...  \n",
       "4919             10  48  64 204 241 281 318 378 451 493  \n",
       "4541             10  48  64 204 241 281 318 378 451 493  \n",
       "4793             10  48  64 204 241 281 318 378 451 493  \n",
       "5045             10  48  64 204 241 281 318 378 451 493  \n",
       "4667             10  48  64 204 241 281 318 378 451 493  \n",
       "4415             10  48  64 204 241 281 318 378 451 493  \n",
       "2398                         10  48  64 204 281 318 493  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sort_values(\"score\", ascending = False).head(30) # with last step standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ead7261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"results-artificial.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cbeb9",
   "metadata": {},
   "source": [
    "# Calculating feature counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "36d639bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "\n",
    "splitted_columns = res.loc[:,\"chosen_columns\"].map(lambda x: x.split(\" \"))\n",
    "\n",
    "for sc in splitted_columns:\n",
    "    aux = np.append(aux, sc)\n",
    "    \n",
    "aux2 = np.unique(aux, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8965e30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['', '10', '105', '128', '153', '204', '241', '28', '281', '298',\n",
       "        '318', '336', '338', '378', '410', '433', '442', '451', '453',\n",
       "        '472', '48', '481', '493', '64'], dtype='<U32'),\n",
       " array([15138,  7056,  5130,  4770,   234,  7056,   612,   162,   738,\n",
       "          288,  1008,  2232,  3906,   576,   144,   126,  1674,   324,\n",
       "         1962,  2826,  1890,  3168,  1656,  6030], dtype=int64))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ecfeda5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15138,  7056,  5130,  4770,   234,  7056,   612,   162,   738,\n",
       "         288,  1008,  2232,  3906,   576,   144,   126,  1674,   324,\n",
       "        1962,  2826,  1890,  3168,  1656,  6030], dtype=int64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "19112993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '204', '64', '105', '128', '338', '481', '472', '336', '453',\n",
       "       '48', '442', '493', '318', '281', '241', '378', '451', '298',\n",
       "       '153', '28', '410', '433'], dtype='<U32')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, count = aux2\n",
    "\n",
    "count_sort_ind = np.argsort(-count)\n",
    "\n",
    "best_columns = u[count_sort_ind][1:]\n",
    "\n",
    "best_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "482c73db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10, 204,  64, 105, 128, 338, 481, 472, 336, 453,  48, 442, 493,\n",
       "       318, 281, 241, 378, 451, 298, 153,  28, 410, 433])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_columns.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d3cc9",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "22c9872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search:\n",
    "\n",
    "learning_rates = [0.001,0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "n_ests = [10,30,50,70,90,100,120,150]\n",
    "max_depths = [3,4,5,6,7,8]\n",
    "\n",
    "ht_cases = np.array(np.meshgrid(learning_rates, n_ests, max_depths)).T.reshape(-1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d15eabc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ht_cases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "55c1eeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 5/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n",
      "m: 6/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n",
      "m: 7/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n",
      "m: 8/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n",
      "m: 9/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n",
      "m: 10/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n",
      "m: 11/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n",
      "m: 12/12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 17.36111111111111 %\n",
      "PROGRESS: 34.72222222222222 %\n",
      "PROGRESS: 52.083333333333336 %\n",
      "PROGRESS: 69.44444444444444 %\n",
      "PROGRESS: 86.80555555555556 %\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(110)\n",
    "\n",
    "data_train = X1_train\n",
    "data_val = X1_val\n",
    "\n",
    "counter = 1\n",
    "res2 = pd.DataFrame({\"lr\": {},\"n_est\":{}, \"max_depth\":{}, \"m\": {}, \"score\": {}})\n",
    "\n",
    "\n",
    "for m in range(5,13):\n",
    "    print(\"m: \" + str(m) + \"/12\")\n",
    "    counter += 1\n",
    "    \n",
    "    # new data for each feature selection case:\n",
    "    data_train = X1_train\n",
    "    data_val = X1_val\n",
    "    \n",
    "    # pick m best columns\n",
    "    \n",
    "    data_train = data_train.iloc[:, best_columns[0:m].astype(int)]\n",
    "    data_val = data_val.iloc[:, best_columns[0:m].astype(int)]\n",
    "\n",
    "    # STANDARDIZING\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler1.fit(data_train)\n",
    "    columns1 = data_train.columns\n",
    "    data_train = pd.DataFrame(scaler1.transform(data_train))\n",
    "    data_val = pd.DataFrame(scaler1.transform(data_val))\n",
    "    data_train.columns = columns1\n",
    "    data_val.columns = columns1    \n",
    "    \n",
    "    # hyperparam tuning\n",
    "    happy_counter = 0\n",
    "    for case in ht_cases:\n",
    "        \n",
    "        if happy_counter % 50 == 0:\n",
    "            print(\"PROGRESS: \" + str(100*happy_counter/len(ht_cases)) + \" %\")\n",
    "        happy_counter += 1\n",
    "        \n",
    "        # getting best model from feature selection previous step\n",
    "        xgboost = GradientBoostingClassifier(learning_rate = case[0], n_estimators = case[1].astype(int), max_depth = case[2].astype(int), random_state = 110)\n",
    "        xgboost.fit(data_train, y1_train)\n",
    "        predictions = xgboost.predict(data_val)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res2 = res2.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": score}, ignore_index = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f50b617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>n_est</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>m</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>0.10</td>\n",
       "      <td>150.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.845012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.844644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0.50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.839224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.837097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>0.05</td>\n",
       "      <td>150.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.835779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>0.10</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.835097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>0.10</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.834856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>0.50</td>\n",
       "      <td>150.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.834615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.50</td>\n",
       "      <td>90.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.833082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0.10</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.833082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.50</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.833082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>0.10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.832969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>0.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.832969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>0.10</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.832841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>0.10</td>\n",
       "      <td>90.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.832728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>0.50</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.832728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>0.10</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.832728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>0.10</td>\n",
       "      <td>150.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.830955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>0.05</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.829878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>0.50</td>\n",
       "      <td>150.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.829196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lr  n_est  max_depth     m     score\n",
       "2295  0.10  150.0        8.0  12.0  0.845012\n",
       "2010  0.50   50.0        8.0  11.0  0.844644\n",
       "2009  0.50   30.0        8.0  11.0  0.839224\n",
       "2013  0.50  100.0        8.0  11.0  0.837097\n",
       "2287  0.05  150.0        8.0  12.0  0.835779\n",
       "2294  0.10  120.0        8.0  12.0  0.835097\n",
       "2291  0.10   70.0        8.0  12.0  0.834856\n",
       "2303  0.50  150.0        8.0  12.0  0.834615\n",
       "2012  0.50   90.0        8.0  11.0  0.833082\n",
       "2003  0.10   70.0        8.0  11.0  0.833082\n",
       "2011  0.50   70.0        8.0  11.0  0.833082\n",
       "2293  0.10  100.0        8.0  12.0  0.832969\n",
       "2301  0.50  100.0        8.0  12.0  0.832969\n",
       "2006  0.10  120.0        8.0  11.0  0.832841\n",
       "2292  0.10   90.0        8.0  12.0  0.832728\n",
       "2302  0.50  120.0        8.0  12.0  0.832728\n",
       "2290  0.10   50.0        8.0  12.0  0.832728\n",
       "2007  0.10  150.0        8.0  11.0  0.830955\n",
       "2286  0.05  120.0        8.0  12.0  0.829878\n",
       "2255  0.50  150.0        7.0  12.0  0.829196"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.sort_values(\"score\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1703f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.to_csv(\"results2-artificial.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd28a8",
   "metadata": {},
   "source": [
    "# Step 2 - hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e645e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search:\n",
    "\n",
    "learning_rates = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "n_ests = [100, 150, 200, 300, 400]\n",
    "max_depths = [7,9,11,13]\n",
    "\n",
    "ht_cases = np.array(np.meshgrid(learning_rates, n_ests, max_depths)).T.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "09e51f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ht_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4cbc7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cases = {\"case1\": best_columns[0:5].astype(int),\n",
    "               \"case2\": best_columns[0:6].astype(int),\n",
    "               \"case3\": best_columns[0:7].astype(int),\n",
    "               \"case4\": best_columns[0:8].astype(int),\n",
    "               \"case5\": best_columns[0:9].astype(int),\n",
    "               \"case6\": best_columns[0:10].astype(int),\n",
    "               \"case7\": best_columns[0:11].astype(int),\n",
    "               \"case8\": best_columns[0:12].astype(int),\n",
    "               \"case9\": best_columns[0:13].astype(int),\n",
    "               \"case10\": best_columns[0:14].astype(int),\n",
    "               \"case11\": best_columns[0:15].astype(int),\n",
    "               \"case12\": [10, 48 ,64, 204, 241, 281, 318, 378, 493],\n",
    "               \"case13\": [10, 64, 105, 204 ,241 ,281 ,318, 378, 453, 481],\n",
    "               \"case14\": [10, 28, 48, 64, 105, 204, 338, 442, 453, 493]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3e95f606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: case1\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case2\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case3\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case4\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case5\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case6\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case7\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case8\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case9\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case10\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case11\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case13\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n",
      "case: case14\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 50.0 %\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(110)\n",
    "\n",
    "data_train = X1_train\n",
    "data_val = X1_val\n",
    "\n",
    "res3 = pd.DataFrame({\"lr\": {},\"n_est\":{}, \"max_depth\":{}, \"m\": {}, \"score\": {}, \"case\": {}, \"standardize\": {}})\n",
    "\n",
    "\n",
    "for case_col, best_cols in column_cases.items():\n",
    "    print(\"case: \" + case_col)\n",
    "    \n",
    "    # new data for each feature selection case:\n",
    "    data_train = X1_train\n",
    "    data_val = X1_val\n",
    "    \n",
    "    # pick m best columns\n",
    "    \n",
    "    data_train = data_train.iloc[:, best_cols]\n",
    "    data_val = data_val.iloc[:, best_cols]\n",
    "\n",
    "    # STANDARDIZING\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler1.fit(data_train)\n",
    "    columns1 = data_train.columns\n",
    "    data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "    data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "    data_trains.columns = columns1\n",
    "    data_vals.columns = columns1    \n",
    "    \n",
    "    # hyperparam tuning\n",
    "    happy_counter = 0\n",
    "    for case in ht_cases:\n",
    "        \n",
    "        if happy_counter % 50 == 0:\n",
    "            print(\"PROGRESS: \" + str(100*happy_counter/len(ht_cases)) + \" %\")\n",
    "        happy_counter += 1\n",
    "        \n",
    "        # getting best model from feature selection previous step\n",
    "        xgboost = GradientBoostingClassifier(learning_rate = case[0], n_estimators = case[1].astype(int), max_depth = case[2].astype(int), random_state = 110)\n",
    "        xgboost.fit(data_train, y1_train)\n",
    "        predictions = xgboost.predict(data_val)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res3 = res3.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": score, \"case\": case_col, \"standardize\": False}, ignore_index = True)\n",
    "\n",
    "        xgboost.fit(data_trains, y1_train)\n",
    "        predictions = xgboost.predict(data_vals)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res3 = res3.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": score, \"case\": case_col, \"standardize\": True}, ignore_index = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1c3f8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3.to_csv(\"results3-artificial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "057a5295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>n_est</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>m</th>\n",
       "      <th>score</th>\n",
       "      <th>case</th>\n",
       "      <th>standardize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>0.3</td>\n",
       "      <td>150.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.882788</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>0.3</td>\n",
       "      <td>150.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.880661</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>0.3</td>\n",
       "      <td>150.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.875723</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>0.3</td>\n",
       "      <td>150.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.873595</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>0.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>case11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>0.1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>0.1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>0.9</td>\n",
       "      <td>150.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>case11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>0.9</td>\n",
       "      <td>150.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>case11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>0.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.872873</td>\n",
       "      <td>case11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>0.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.872873</td>\n",
       "      <td>case11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>0.1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.872873</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>0.3</td>\n",
       "      <td>400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.871428</td>\n",
       "      <td>case11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>0.3</td>\n",
       "      <td>400.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.871428</td>\n",
       "      <td>case11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>0.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.871227</td>\n",
       "      <td>case11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.871227</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0.9</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.870986</td>\n",
       "      <td>case11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>0.9</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.870986</td>\n",
       "      <td>case11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>0.5</td>\n",
       "      <td>300.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.870745</td>\n",
       "      <td>case11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr  n_est  max_depth     m     score    case  standardize\n",
       "2763  0.3  150.0       13.0  12.0  0.882788  case14          1.0\n",
       "2762  0.3  150.0       13.0  12.0  0.880661  case14          0.0\n",
       "2713  0.3  150.0       11.0  12.0  0.875723  case14          1.0\n",
       "2712  0.3  150.0       11.0  12.0  0.873595  case14          0.0\n",
       "2130  0.7  100.0       11.0  12.0  0.873114  case11          0.0\n",
       "2707  0.1  300.0       11.0  12.0  0.873114  case14          1.0\n",
       "2706  0.1  300.0       11.0  12.0  0.873114  case14          0.0\n",
       "2704  0.1  200.0       11.0  12.0  0.873114  case14          0.0\n",
       "2042  0.9  150.0        7.0  12.0  0.873114  case11          0.0\n",
       "2043  0.9  150.0        7.0  12.0  0.873114  case11          1.0\n",
       "2040  0.9  100.0        7.0  12.0  0.872873  case11          0.0\n",
       "2041  0.9  100.0        7.0  12.0  0.872873  case11          1.0\n",
       "2709  0.1  400.0       11.0  12.0  0.872873  case14          1.0\n",
       "2068  0.3  400.0        9.0  12.0  0.871428  case11          0.0\n",
       "2069  0.3  400.0        9.0  12.0  0.871428  case11          1.0\n",
       "2131  0.7  100.0       11.0  12.0  0.871227  case11          1.0\n",
       "2705  0.1  200.0       11.0  12.0  0.871227  case14          1.0\n",
       "2044  0.9  200.0        7.0  12.0  0.870986  case11          0.0\n",
       "2045  0.9  200.0        7.0  12.0  0.870986  case11          1.0\n",
       "2026  0.5  300.0        7.0  12.0  0.870745  case11          0.0"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3.sort_values(\"score\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba7d23",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f862a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search:\n",
    "\n",
    "learning_rates = [0.23, 0.25, 0.28, 0.3, 0.32, 0.35, 0.38]\n",
    "n_ests = [145, 150, 155]\n",
    "max_depths = [13,14,15,16,17,18,19,20,21,22]\n",
    "\n",
    "ht_cases = np.array(np.meshgrid(learning_rates, n_ests, max_depths)).T.reshape(-1,3)\n",
    "\n",
    "column_cases = {\"case14\": [10, 28, 48, 64, 105, 204, 338, 442, 453, 493]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ec9e84ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ht_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e1a1cf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: case14\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 23.80952380952381 %\n",
      "PROGRESS: 47.61904761904762 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "PROGRESS: 95.23809523809524 %\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(110)\n",
    "\n",
    "data_train = X1_train\n",
    "data_val = X1_val\n",
    "\n",
    "res3_2 = pd.DataFrame({\"lr\": {},\"n_est\":{}, \"max_depth\":{}, \"m\": {}, \"score\": {}, \"case\": {}, \"standardize\": {}})\n",
    "\n",
    "\n",
    "for case_col, best_cols in column_cases.items():\n",
    "    print(\"case: \" + case_col)\n",
    "    \n",
    "    # new data for each feature selection case:\n",
    "    data_train = X1_train\n",
    "    data_val = X1_val\n",
    "    \n",
    "    # pick m best columns\n",
    "    \n",
    "    data_train = data_train.iloc[:, best_cols]\n",
    "    data_val = data_val.iloc[:, best_cols]\n",
    "\n",
    "    # STANDARDIZING\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler1.fit(data_train)\n",
    "    columns1 = data_train.columns\n",
    "    data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "    data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "    data_trains.columns = columns1\n",
    "    data_vals.columns = columns1    \n",
    "    \n",
    "    # hyperparam tuning\n",
    "    happy_counter = 0\n",
    "    for case in ht_cases:\n",
    "        \n",
    "        if happy_counter % 50 == 0:\n",
    "            print(\"PROGRESS: \" + str(100*happy_counter/len(ht_cases)) + \" %\")\n",
    "        happy_counter += 1\n",
    "        \n",
    "        # getting best model from feature selection previous step\n",
    "        xgboost = GradientBoostingClassifier(learning_rate = case[0], n_estimators = case[1].astype(int), max_depth = case[2].astype(int), random_state = 110)\n",
    "        xgboost.fit(data_train, y1_train)\n",
    "        predictions = xgboost.predict(data_val)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res3_2 = res3_2.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": score, \"case\": case_col, \"standardize\": False}, ignore_index = True)\n",
    "\n",
    "        xgboost.fit(data_trains, y1_train)\n",
    "        predictions = xgboost.predict(data_vals)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res3_2 = res3_2.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": score, \"case\": case_col, \"standardize\": True}, ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "de07fca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>n_est</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>m</th>\n",
       "      <th>score</th>\n",
       "      <th>case</th>\n",
       "      <th>standardize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.30</td>\n",
       "      <td>150.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.882788</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.30</td>\n",
       "      <td>155.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.880661</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.30</td>\n",
       "      <td>150.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.880661</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.30</td>\n",
       "      <td>145.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.880661</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.30</td>\n",
       "      <td>155.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.878533</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.30</td>\n",
       "      <td>145.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.878533</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.38</td>\n",
       "      <td>155.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.38</td>\n",
       "      <td>155.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.826345</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.38</td>\n",
       "      <td>150.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.825181</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.38</td>\n",
       "      <td>155.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.824940</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr  n_est  max_depth     m     score    case  standardize\n",
       "21  0.30  150.0       13.0  12.0  0.882788  case14          1.0\n",
       "23  0.30  155.0       13.0  12.0  0.880661  case14          1.0\n",
       "20  0.30  150.0       13.0  12.0  0.880661  case14          0.0\n",
       "19  0.30  145.0       13.0  12.0  0.880661  case14          1.0\n",
       "22  0.30  155.0       13.0  12.0  0.878533  case14          0.0\n",
       "18  0.30  145.0       13.0  12.0  0.878533  case14          0.0\n",
       "41  0.38  155.0       13.0  12.0  0.827068  case14          1.0\n",
       "83  0.38  155.0       14.0  12.0  0.826345  case14          1.0\n",
       "39  0.38  150.0       13.0  12.0  0.825181  case14          1.0\n",
       "40  0.38  155.0       13.0  12.0  0.824940  case14          0.0"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3_2.sort_values(\"score\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c229c41a",
   "metadata": {},
   "source": [
    "# Optimal train_set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0ef5a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "\n",
    "training_sizes = [0.7, 0.8, 0.9,0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "4965422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 0.05\n",
      "train_size: 0.1\n",
      "train_size: 0.15\n",
      "train_size: 0.2\n",
      "train_size: 0.25\n",
      "train_size: 0.3\n",
      "train_size: 0.35\n",
      "train_size: 0.4\n",
      "train_size: 0.45\n",
      "train_size: 0.5\n",
      "train_size: 0.55\n",
      "train_size: 0.6\n",
      "train_size: 0.65\n",
      "train_size: 0.7\n",
      "train_size: 0.75\n",
      "train_size: 0.8\n",
      "train_size: 0.85\n",
      "train_size: 0.9\n",
      "train_size: 0.95\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#random.seed(110)\n",
    "\n",
    "best_cols = [10, 28, 48, 64, 105, 204, 338, 442, 453, 493]\n",
    "\n",
    "res4 = pd.DataFrame({\"train_size\":{}, \"score\": {}, \"standardize\": {}})\n",
    "m = len(best_cols)\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "for ts in training_sizes:\n",
    "    \n",
    "    print(\"train_size: \" + str(ts))\n",
    "    \n",
    "    for i in range(30):\n",
    "        data_train, data_val, y1_train, y1_val =  train_test_split(Xa_train, ya_train,train_size = ts)\n",
    "\n",
    "        # pick m best columns\n",
    "        data_train = data_train.iloc[:, best_cols]\n",
    "        data_val = data_val.iloc[:, best_cols]\n",
    "\n",
    "        # STANDARDIZING\n",
    "\n",
    "        scaler1.fit(data_train)\n",
    "        columns1 = data_train.columns\n",
    "        data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "        data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "        data_trains.columns = columns1\n",
    "        data_vals.columns = columns1    \n",
    "\n",
    "\n",
    "        # getting best model from feature selection previous step\n",
    "        xgboost = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 50, max_depth = 10)\n",
    "        xgboost.fit(data_train, y1_train)\n",
    "        predictions = xgboost.predict(data_val)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res4 = res4.append({\"train_size\": ts,  \"score\": score, \"standardize\": False}, ignore_index = True)\n",
    "\n",
    "        xgboost.fit(data_trains, y1_train)\n",
    "        predictions = xgboost.predict(data_vals)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res4 = res4.append({\"train_size\": ts,  \"score\": score, \"standardize\": True}, ignore_index = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "a0d11eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 0.7\n",
      "train_size: 0.8\n",
      "train_size: 0.9\n",
      "train_size: 0.95\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#random.seed(110)\n",
    "\n",
    "best_cols = [10, 28, 48, 64, 105, 204, 338, 442, 453, 493]\n",
    "\n",
    "res5 = pd.DataFrame({\"train_size\":{}, \"score\": {}, \"standardize\": {}})\n",
    "m = len(best_cols)\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "for ts in training_sizes:\n",
    "    \n",
    "    print(\"train_size: \" + str(ts))\n",
    "    \n",
    "    for i in range(30):\n",
    "        data_train, data_val, y1_train, y1_val =  train_test_split(Xa_train, ya_train,train_size = ts)\n",
    "\n",
    "        # pick m best columns\n",
    "        data_train = data_train.iloc[:, best_cols]\n",
    "        data_val = data_val.iloc[:, best_cols]\n",
    "\n",
    "        # STANDARDIZING\n",
    "\n",
    "        scaler1.fit(data_train)\n",
    "        columns1 = data_train.columns\n",
    "        data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "        data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "        data_trains.columns = columns1\n",
    "        data_vals.columns = columns1    \n",
    "\n",
    "\n",
    "        # getting best model from feature selection previous step\n",
    "        xgboost = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 50, max_depth = 10)\n",
    "        xgboost.fit(data_train, y1_train)\n",
    "        predictions = xgboost.predict(data_val)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res5 = res5.append({\"train_size\": ts,  \"score\": score, \"standardize\": False}, ignore_index = True)\n",
    "\n",
    "        xgboost.fit(data_trains, y1_train)\n",
    "        predictions = xgboost.predict(data_vals)\n",
    "        score = score1(predictions, y1_val, m)\n",
    "\n",
    "        res5 = res5.append({\"train_size\": ts,  \"score\": score, \"standardize\": True}, ignore_index = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5ef0bb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_size</th>\n",
       "      <th>standardize</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.95</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.864044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.858357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.90</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.851485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.850095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.80</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.849704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.848740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.70</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.842715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.841353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "train_size standardize          \n",
       "0.95       0.0          0.864044\n",
       "           1.0          0.858357\n",
       "0.90       0.0          0.851485\n",
       "           1.0          0.850095\n",
       "0.80       1.0          0.849704\n",
       "           0.0          0.848740\n",
       "0.70       1.0          0.842715\n",
       "           0.0          0.841353"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = res5.groupby([\"train_size\", \"standardize\"])[\"score\"].agg(np.mean)\n",
    "\n",
    "pd.DataFrame(df).sort_values(\"score\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e7eb4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res4.to_csv(\"results4-artificial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5d5b3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res4.groupby([\"train_size\", \"standardize\"])[\"score\"].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "36f5bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df).sort_values(\"score\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "903d9a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_size</th>\n",
       "      <th>standardize</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.95</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.832304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.824567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.817705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.808058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.807590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.85</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.804915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.801299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.797420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.70</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.795908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.795809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.75</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.789277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.786021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.65</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.778310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.775822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.60</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.768829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.768503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.767745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.760478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.759001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.758510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.45</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.746401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.741296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.40</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.741281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.741232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.35</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.735516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.731137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.30</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.718907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.718030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.25</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.694623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.693199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.20</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.674570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.673837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.15</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.659775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.10</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.626510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.625275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.05</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.599750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.598487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "train_size standardize          \n",
       "0.95       0.0          0.832304\n",
       "           1.0          0.824567\n",
       "0.90       0.0          0.817705\n",
       "0.80       1.0          0.808058\n",
       "0.90       1.0          0.807590\n",
       "0.85       1.0          0.804915\n",
       "           0.0          0.801299\n",
       "0.80       0.0          0.797420\n",
       "0.70       0.0          0.795908\n",
       "           1.0          0.795809\n",
       "0.75       0.0          0.789277\n",
       "           1.0          0.786021\n",
       "0.65       1.0          0.778310\n",
       "           0.0          0.775822\n",
       "0.60       1.0          0.768829\n",
       "           0.0          0.768503\n",
       "0.55       1.0          0.767745\n",
       "0.50       1.0          0.760478\n",
       "0.55       0.0          0.759001\n",
       "0.50       0.0          0.758510\n",
       "0.45       1.0          0.746401\n",
       "           0.0          0.741296\n",
       "0.40       0.0          0.741281\n",
       "           1.0          0.741232\n",
       "0.35       1.0          0.735516\n",
       "           0.0          0.731137\n",
       "0.30       1.0          0.718907\n",
       "           0.0          0.718030\n",
       "0.25       0.0          0.694623\n",
       "           1.0          0.693199\n",
       "0.20       0.0          0.674570\n",
       "           1.0          0.673837\n",
       "0.15       0.0          0.659775\n",
       "           1.0          0.658700\n",
       "0.10       1.0          0.626510\n",
       "           0.0          0.625275\n",
       "0.05       1.0          0.599750\n",
       "           0.0          0.598487"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "21cf1ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th>score</th>\n",
       "      <th>standardize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.886788</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.884661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.872870</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.872870</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.862747</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.595412</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.595368</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.595368</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.591056</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_size     score  standardize\n",
       "85        0.75  0.886788          1.0\n",
       "84        0.75  0.884661          0.0\n",
       "96        0.85  0.872870          0.0\n",
       "97        0.85  0.872870          1.0\n",
       "99        0.85  0.862747          1.0\n",
       "..         ...       ...          ...\n",
       "10        0.10  0.595412          0.0\n",
       "7         0.10  0.595368          1.0\n",
       "6         0.10  0.595368          0.0\n",
       "8         0.10  0.591056          0.0\n",
       "9         0.10  0.589953          1.0\n",
       "\n",
       "[114 rows x 3 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res4.sort_values(\"score\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bad1a",
   "metadata": {},
   "source": [
    "Model seems to be overfitting - Score of 0.88 is too optimistic, and max_depth too high to be used in test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d573041",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning - omitting random seed (bacause of overfitting on validation dataset), replacing it with average of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a32bc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search:\n",
    "\n",
    "learning_rates = [0.05, 0.15, 0.3, 0.5]\n",
    "n_ests = [10,20,30,40,50,60,70]\n",
    "max_depths = [3,4,5,6,7]\n",
    "\n",
    "ht_cases = np.array(np.meshgrid(learning_rates, n_ests, max_depths)).T.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1d58d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cases = {\"case1\": best_columns[0:5].astype(int),\n",
    "               \"case2\": best_columns[0:6].astype(int),\n",
    "               \"case3\": best_columns[0:7].astype(int),\n",
    "               \"case4\": best_columns[0:8].astype(int),\n",
    "               \"case5\": best_columns[0:9].astype(int),\n",
    "               \"case6\": best_columns[0:10].astype(int),\n",
    "               \"case7\": best_columns[0:11].astype(int),\n",
    "               \"case8\": best_columns[0:12].astype(int),\n",
    "               \"case9\": best_columns[0:13].astype(int),\n",
    "               \"case10\": best_columns[0:14].astype(int),\n",
    "               \"case11\": best_columns[0:15].astype(int),\n",
    "               \"case12\": [10, 48 ,64, 204, 241, 281, 318, 378, 493],\n",
    "               \"case13\": [10, 64, 105, 204 ,241 ,281 ,318, 378, 453, 481],\n",
    "               \"case14\": [10, 28, 48, 64, 105, 204, 338, 442, 453, 493]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "6a20ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: case1\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case2\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case3\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case4\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case5\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case6\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case7\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case8\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case9\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case10\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case11\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case12\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case13\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n",
      "case: case14\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 35.714285714285715 %\n",
      "PROGRESS: 71.42857142857143 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# grid search:\n",
    "\n",
    "learning_rates = [0.05, 0.15, 0.3, 0.5]\n",
    "n_ests = [10,20,30,40,50,60,70]\n",
    "max_depths = [3,4,5,6,7]\n",
    "\n",
    "ht_cases = np.array(np.meshgrid(learning_rates, n_ests, max_depths)).T.reshape(-1,3)\n",
    "\n",
    "column_cases = {\"case1\": best_columns[0:5].astype(int),\n",
    "               \"case2\": best_columns[0:6].astype(int),\n",
    "               \"case3\": best_columns[0:7].astype(int),\n",
    "               \"case4\": best_columns[0:8].astype(int),\n",
    "               \"case5\": best_columns[0:9].astype(int),\n",
    "               \"case6\": best_columns[0:10].astype(int),\n",
    "               \"case7\": best_columns[0:11].astype(int),\n",
    "               \"case8\": best_columns[0:12].astype(int),\n",
    "               \"case9\": best_columns[0:13].astype(int),\n",
    "               \"case10\": best_columns[0:14].astype(int),\n",
    "               \"case11\": best_columns[0:15].astype(int),\n",
    "               \"case12\": [10, 48 ,64, 204, 241, 281, 318, 378, 493],\n",
    "               \"case13\": [10, 64, 105, 204 ,241 ,281 ,318, 378, 453, 481],\n",
    "               \"case14\": [10, 28, 48, 64, 105, 204, 338, 442, 453, 493]}\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# random.seed(110)\n",
    "\n",
    "res5_2 = pd.DataFrame({\"lr\": {},\"n_est\":{}, \"max_depth\":{}, \"m\": {}, \"score\": {}, \"case\": {}, \"standardize\": {}})\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "for case_col, best_cols in column_cases.items():\n",
    "    print(\"case: \" + case_col)\n",
    "    \n",
    "    m = len(best_cols)\n",
    "\n",
    "    # STANDARDIZING\n",
    "    \n",
    "    #     scaler1.fit(data_train)\n",
    "    #     columns1 = data_train.columns\n",
    "    #     data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "    #     data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "    #     data_trains.columns = columns1\n",
    "    #     data_vals.columns = columns1    \n",
    "    \n",
    "    # hyperparam tuning\n",
    "    happy_counter = 0\n",
    "    for case in ht_cases:\n",
    "        \n",
    "        if happy_counter % 50 == 0:\n",
    "            print(\"PROGRESS: \" + str(100*happy_counter/len(ht_cases)) + \" %\")\n",
    "        happy_counter += 1\n",
    "\n",
    "        scores = []\n",
    "        scoress = []\n",
    "\n",
    "        for i in range(30):\n",
    "            \n",
    "            data_train_, data_val_, y1_train, y1_val =  train_test_split(Xa_train, ya_train,train_size = ts)\n",
    "\n",
    "            # pick m best columns\n",
    "            data_train_ = data_train_.iloc[:, best_cols]\n",
    "            data_val_ = data_val_.iloc[:, best_cols]\n",
    "            \n",
    "            # STANDARDIZING\n",
    "\n",
    "            scaler1.fit(data_train_)\n",
    "            columns1 = data_train_.columns\n",
    "            data_trains = pd.DataFrame(scaler1.transform(data_train_))\n",
    "            data_vals = pd.DataFrame(scaler1.transform(data_val_))\n",
    "            data_trains.columns = columns1\n",
    "            data_vals.columns = columns1   \n",
    "\n",
    "            \n",
    "\n",
    "            # getting best model from feature selection previous step\n",
    "            xgboost = GradientBoostingClassifier(learning_rate = case[0], n_estimators = case[1].astype(int), max_depth = case[2].astype(int))\n",
    "            xgboost.fit(data_train_, y1_train)\n",
    "            predictions = xgboost.predict(data_val_)\n",
    "            score = score1(predictions, y1_val, m)\n",
    "            scores.append(score)\n",
    "\n",
    "\n",
    "            xgboost.fit(data_trains, y1_train)\n",
    "            predictions = xgboost.predict(data_vals)\n",
    "            score = score1(predictions, y1_val, m)\n",
    "            scoress.append(score)\n",
    "            \n",
    "        res5_2 = res5_2.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": np.mean(scores), \"case\": case_col, \"standardize\": False}, ignore_index = True)\n",
    "        res5_2 = res5_2.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": np.mean(scoress), \"case\": case_col, \"standardize\": True}, ignore_index = True)\n",
    "\n",
    "\n",
    "res5_2.to_csv(\"results5_2-artificial.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "67623873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>n_est</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>m</th>\n",
       "      <th>score</th>\n",
       "      <th>case</th>\n",
       "      <th>standardize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>0.15</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.863337</td>\n",
       "      <td>case14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>0.15</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.861726</td>\n",
       "      <td>case10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>0.15</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.859985</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>0.30</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.859855</td>\n",
       "      <td>case13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>0.15</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.858655</td>\n",
       "      <td>case14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.582403</td>\n",
       "      <td>case1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.50</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.581454</td>\n",
       "      <td>case1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.30</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.579912</td>\n",
       "      <td>case1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.30</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.579570</td>\n",
       "      <td>case1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.50</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.575802</td>\n",
       "      <td>case1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3920 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lr  n_est  max_depth     m     score    case  standardize\n",
       "3891  0.15   70.0        7.0  10.0  0.863337  case14          1.0\n",
       "2765  0.15   40.0        7.0  14.0  0.861726  case10          1.0\n",
       "3888  0.15   60.0        7.0  10.0  0.859985  case14          0.0\n",
       "3621  0.30   50.0        7.0  10.0  0.859855  case13          1.0\n",
       "3890  0.15   70.0        7.0  10.0  0.858655  case14          0.0\n",
       "...    ...    ...        ...   ...       ...     ...          ...\n",
       "0     0.05   10.0        3.0   5.0  0.582403   case1          0.0\n",
       "273   0.50   40.0        7.0   5.0  0.581454   case1          1.0\n",
       "28    0.30   10.0        3.0   5.0  0.579912   case1          0.0\n",
       "29    0.30   10.0        3.0   5.0  0.579570   case1          1.0\n",
       "271   0.50   30.0        7.0   5.0  0.575802   case1          1.0\n",
       "\n",
       "[3920 rows x 7 columns]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res5_2.sort_values(\"score\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6c6f0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special version for digits dataset - removes features with p value = 0 (there was plenty of them)\n",
    "def rm2KofFeaturesFromNormalDistribution2(data_train, data_test):\n",
    "    tt = normaltest(data_train)\n",
    "    p_values = tt[1]\n",
    "\n",
    "    aux = data_train.columns[np.where(p_values == 0)]\n",
    "\n",
    "    tt = normaltest(data_test)\n",
    "    p_values = tt[1]\n",
    "\n",
    "    aux2 = data_test.columns[np.where(p_values == 0)]\n",
    "\n",
    "    x = np.append(aux, aux2)\n",
    "    x = np.unique(x)\n",
    "\n",
    "    return data_train.drop(x,axis = 1), data_test.drop(x,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f20c65ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "variance_ks = [0,10,20,40,80,150,300, 500]\n",
    "normality_ks = [True, False]\n",
    "boruta_flags = [True] # always use boruta\n",
    "vif_ks = [50,52,54,56,58,60, 65, 70, 80, 100]\n",
    "\n",
    "feature_selection_cases = np.array(np.meshgrid(variance_ks, normality_ks, boruta_flags, vif_ks)).T.reshape(-1,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7ae9dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE: 1/160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/2803913261.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapplyBorutaSelection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myd_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboruta_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleaveKusingVIF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mchosen_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/2124656383.py\u001b[0m in \u001b[0;36mleaveKusingVIF\u001b[1;34m(data_train, data_test, k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mleaveKusingVIF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         ds=pd.Series([variance_inflation_factor(data_train.values, i)   \n\u001b[0m\u001b[0;32m      4\u001b[0m            for i in range(data_train.shape[1])],   \n\u001b[0;32m      5\u001b[0m               index=data_train.columns)  \n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/2124656383.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mleaveKusingVIF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         ds=pd.Series([variance_inflation_factor(data_train.values, i)   \n\u001b[0m\u001b[0;32m      4\u001b[0m            for i in range(data_train.shape[1])],   \n\u001b[0;32m      5\u001b[0m               index=data_train.columns)  \n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py\u001b[0m in \u001b[0;36mvariance_inflation_factor\u001b[1;34m(exog, exog_idx)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_vars\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexog_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[0mx_noti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mr_squared_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_noti\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsquared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m     \u001b[0mvif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mr_squared_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvif\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpinv_wexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingular_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpinv_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwexog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                 self.normalized_cov_params = np.dot(\n\u001b[0m\u001b[0;32m    307\u001b[0m                     self.pinv_wexog, np.transpose(self.pinv_wexog))\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do not use - very slow - skip it\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#random.seed(110)\n",
    "\n",
    "\n",
    "# initial model for boruta\n",
    "boruta_model = RandomForestClassifier(n_estimators = 40, random_state = 110, n_jobs= -1, max_depth = 5)\n",
    "\n",
    "counter = 1\n",
    "res6 = pd.DataFrame({\"m\": {}, \"model_name\": {},\"variance_k\":{}, \"normality_k\":{}, \"boruta_flag\": {}, \"vif_k\":{}, \"score\": {}, \"chosen_columns\": {}, \"standardize\": {}})\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "for case in feature_selection_cases:\n",
    "    print(\"CASE: \" + str(counter) + \"/160\")\n",
    "    counter += 1\n",
    "    \n",
    "    # new data for each feature selection case:\n",
    "    data_train = Xd_train\n",
    "    data_val = Xd_test\n",
    "    \n",
    "    # FEATURE SELECTION METHODS:\n",
    "    data_train, data_val = rm2KofLowestVariance(data_train, data_val, case[0])\n",
    "    if case[1]:\n",
    "        data_train, data_val = rm2KofFeaturesFromNormalDistribution2(data_train, data_val )\n",
    "    if case[2]:\n",
    "        data_train, data_val = applyBorutaSelection(data_train, yd_train, data_val, boruta_model)\n",
    "    data_train, data_val = leaveKusingVIF(data_train, data_val, case[3])\n",
    "    \n",
    "    chosen_columns = str(data_train.columns.values)[1:-1]\n",
    "    best_cols = data_train.columns.values\n",
    "    \n",
    "    m = data_train.shape[1]\n",
    "    \n",
    "    scores = []\n",
    "    scoress = []\n",
    "    \n",
    "    print(\"Started ML phase! (30 iterations)\")\n",
    "\n",
    "    for i in range(30):\n",
    "\n",
    "        data_train, data_val, y1_train, y1_val =  train_test_split(Xd_train, yd_train,train_size = 0.8)\n",
    "\n",
    "        # pick m best columns\n",
    "        data_train = data_train.iloc[:, best_cols]\n",
    "        data_val = data_val.iloc[:, best_cols]\n",
    "\n",
    "        # STANDARDIZING\n",
    "\n",
    "        scaler1.fit(data_train)\n",
    "        columns1 = data_train.columns\n",
    "        data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "        data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "        data_trains.columns = columns1\n",
    "        data_vals.columns = columns1   \n",
    "\n",
    "\n",
    "\n",
    "        # getting best model from feature selection previous step\n",
    "        xgboost = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 40, max_depth = 5)\n",
    "        xgboost.fit(data_train, y1_train)\n",
    "        predictions = xgboost.predict(data_val)\n",
    "        score = score2(predictions, y1_val, m)\n",
    "        scores.append(score)\n",
    "\n",
    "\n",
    "        xgboost.fit(data_trains, y1_train)\n",
    "        predictions = xgboost.predict(data_vals)\n",
    "        score = score2(predictions, y1_val, m)\n",
    "        scoress.append(score)\n",
    "\n",
    "    res6 = res6.append({\"m\": m,\"model_name\": \"xgb\", \"variance_k\": case[0], \"normality_k\": case[1], \"boruta_flag\": case[2], \"vif_k\":case[3], \"score\": np.mean(scoress), \"chosen_columns\": chosen_columns, \"standardize\": True}, ignore_index = True)\n",
    "    res6 = res6.append({\"m\": m,\"model_name\": \"xgb\", \"variance_k\": case[0], \"normality_k\": case[1], \"boruta_flag\": case[2], \"vif_k\":case[3], \"score\": np.mean(scores), \"chosen_columns\": chosen_columns, \"standardize\": False}, ignore_index = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2d79b4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 4698)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "7c614c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 606)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "aaa943f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyBorutaSelection_(data_train, labels, data_test, classifModel):\n",
    "    # define Boruta feature selection method\n",
    "    feat_selector = BorutaPy(classifModel, n_estimators='auto', random_state=110, verbose=2)\n",
    "\n",
    "    # find all relevant features - 5 features should be selected\n",
    "    feat_selector.fit(data_train.values, labels.values.ravel())\n",
    "\n",
    "    return data_train.iloc[:,feat_selector.support_], data_test.iloc[:,feat_selector.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "0d4f98a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t606\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t606\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t606\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t606\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t606\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t606\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t606\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t258\n",
      "Tentative: \t33\n",
      "Rejected: \t315\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t258\n",
      "Tentative: \t33\n",
      "Rejected: \t315\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t258\n",
      "Tentative: \t33\n",
      "Rejected: \t315\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t258\n",
      "Tentative: \t33\n",
      "Rejected: \t315\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t261\n",
      "Tentative: \t30\n",
      "Rejected: \t315\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t261\n",
      "Tentative: \t30\n",
      "Rejected: \t315\n",
      "Iteration: \t14 / 100\n",
      "Confirmed: \t261\n",
      "Tentative: \t30\n",
      "Rejected: \t315\n",
      "Iteration: \t15 / 100\n",
      "Confirmed: \t261\n",
      "Tentative: \t27\n",
      "Rejected: \t318\n",
      "Iteration: \t16 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t25\n",
      "Rejected: \t318\n",
      "Iteration: \t17 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t25\n",
      "Rejected: \t318\n",
      "Iteration: \t18 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t25\n",
      "Rejected: \t318\n",
      "Iteration: \t19 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t25\n",
      "Rejected: \t318\n",
      "Iteration: \t20 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t25\n",
      "Rejected: \t318\n",
      "Iteration: \t21 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t24\n",
      "Rejected: \t319\n",
      "Iteration: \t22 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t24\n",
      "Rejected: \t319\n",
      "Iteration: \t23 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t22\n",
      "Rejected: \t321\n",
      "Iteration: \t24 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t22\n",
      "Rejected: \t321\n",
      "Iteration: \t25 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t22\n",
      "Rejected: \t321\n",
      "Iteration: \t26 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t22\n",
      "Rejected: \t321\n",
      "Iteration: \t27 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t21\n",
      "Rejected: \t322\n",
      "Iteration: \t28 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t21\n",
      "Rejected: \t322\n",
      "Iteration: \t29 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t21\n",
      "Rejected: \t322\n",
      "Iteration: \t30 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t21\n",
      "Rejected: \t322\n",
      "Iteration: \t31 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t21\n",
      "Rejected: \t322\n",
      "Iteration: \t32 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t19\n",
      "Rejected: \t324\n",
      "Iteration: \t33 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t19\n",
      "Rejected: \t324\n",
      "Iteration: \t34 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t19\n",
      "Rejected: \t324\n",
      "Iteration: \t35 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t19\n",
      "Rejected: \t324\n",
      "Iteration: \t36 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t37 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t38 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t39 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t40 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t41 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t42 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t43 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t44 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t45 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t46 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t47 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t48 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t49 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t50 / 100\n",
      "Confirmed: \t263\n",
      "Tentative: \t18\n",
      "Rejected: \t325\n",
      "Iteration: \t51 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t17\n",
      "Rejected: \t325\n",
      "Iteration: \t52 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t17\n",
      "Rejected: \t325\n",
      "Iteration: \t53 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t17\n",
      "Rejected: \t325\n",
      "Iteration: \t54 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t17\n",
      "Rejected: \t325\n",
      "Iteration: \t55 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t17\n",
      "Rejected: \t325\n",
      "Iteration: \t56 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t17\n",
      "Rejected: \t325\n",
      "Iteration: \t57 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t17\n",
      "Rejected: \t325\n",
      "Iteration: \t58 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t59 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t60 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t61 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t62 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t63 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t64 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t65 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t16\n",
      "Rejected: \t326\n",
      "Iteration: \t66 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t15\n",
      "Rejected: \t327\n",
      "Iteration: \t67 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t15\n",
      "Rejected: \t327\n",
      "Iteration: \t68 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t69 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t70 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t71 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t72 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t73 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t74 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t75 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t76 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t77 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t78 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t79 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t80 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t81 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t82 / 100\n",
      "Confirmed: \t264\n",
      "Tentative: \t14\n",
      "Rejected: \t328\n",
      "Iteration: \t83 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t84 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t85 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t86 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t87 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t88 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t89 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t90 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t13\n",
      "Rejected: \t328\n",
      "Iteration: \t91 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t11\n",
      "Rejected: \t330\n",
      "Iteration: \t92 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t11\n",
      "Rejected: \t330\n",
      "Iteration: \t93 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t11\n",
      "Rejected: \t330\n",
      "Iteration: \t94 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t11\n",
      "Rejected: \t330\n",
      "Iteration: \t95 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t11\n",
      "Rejected: \t330\n",
      "Iteration: \t96 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t10\n",
      "Rejected: \t331\n",
      "Iteration: \t97 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t10\n",
      "Rejected: \t331\n",
      "Iteration: \t98 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t10\n",
      "Rejected: \t331\n",
      "Iteration: \t99 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t10\n",
      "Rejected: \t331\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t100 / 100\n",
      "Confirmed: \t265\n",
      "Tentative: \t2\n",
      "Rejected: \t331\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/3243547849.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapplyBorutaSelection_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myd_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboruta_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleaveKusingVIF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/2124656383.py\u001b[0m in \u001b[0;36mleaveKusingVIF\u001b[1;34m(data_train, data_test, k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mleaveKusingVIF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         ds=pd.Series([variance_inflation_factor(data_train.values, i)   \n\u001b[0m\u001b[0;32m      4\u001b[0m            for i in range(data_train.shape[1])],   \n\u001b[0;32m      5\u001b[0m               index=data_train.columns)  \n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/2124656383.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mleaveKusingVIF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         ds=pd.Series([variance_inflation_factor(data_train.values, i)   \n\u001b[0m\u001b[0;32m      4\u001b[0m            for i in range(data_train.shape[1])],   \n\u001b[0;32m      5\u001b[0m               index=data_train.columns)  \n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py\u001b[0m in \u001b[0;36mvariance_inflation_factor\u001b[1;34m(exog, exog_idx)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0mx_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_vars\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexog_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[0mx_noti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[0mr_squared_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_noti\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsquared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mvif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mr_squared_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_train, data_val = rm2KofLowestVariance(Xd_train, Xd_test, 500)\n",
    "data_train, data_val = rm2KofFeaturesFromNormalDistribution2(data_train, data_val )\n",
    "data_train, data_val = applyBorutaSelection_(data_train, yd_train, data_val, boruta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "389ede06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vif_k: 260\n",
      "Started ML phase! (10 iterations), m = 260\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 255\n",
      "Started ML phase! (10 iterations), m = 255\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 250\n",
      "Started ML phase! (10 iterations), m = 250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 245\n",
      "Started ML phase! (10 iterations), m = 245\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 240\n",
      "Started ML phase! (10 iterations), m = 240\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 235\n",
      "Started ML phase! (10 iterations), m = 235\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 230\n",
      "Started ML phase! (10 iterations), m = 230\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 225\n",
      "Started ML phase! (10 iterations), m = 225\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 220\n",
      "Started ML phase! (10 iterations), m = 220\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 215\n",
      "Started ML phase! (10 iterations), m = 215\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 210\n",
      "Started ML phase! (10 iterations), m = 210\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 205\n",
      "Started ML phase! (10 iterations), m = 205\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 200\n",
      "Started ML phase! (10 iterations), m = 200\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 195\n",
      "Started ML phase! (10 iterations), m = 195\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 190\n",
      "Started ML phase! (10 iterations), m = 190\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 185\n",
      "Started ML phase! (10 iterations), m = 185\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 180\n",
      "Started ML phase! (10 iterations), m = 180\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 175\n",
      "Started ML phase! (10 iterations), m = 175\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 170\n",
      "Started ML phase! (10 iterations), m = 170\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 165\n",
      "Started ML phase! (10 iterations), m = 165\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 160\n",
      "Started ML phase! (10 iterations), m = 160\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 155\n",
      "Started ML phase! (10 iterations), m = 155\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 150\n",
      "Started ML phase! (10 iterations), m = 150\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 145\n",
      "Started ML phase! (10 iterations), m = 145\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 140\n",
      "Started ML phase! (10 iterations), m = 140\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 135\n",
      "Started ML phase! (10 iterations), m = 135\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 130\n",
      "Started ML phase! (10 iterations), m = 130\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 125\n",
      "Started ML phase! (10 iterations), m = 125\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 120\n",
      "Started ML phase! (10 iterations), m = 120\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 115\n",
      "Started ML phase! (10 iterations), m = 115\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 110\n",
      "Started ML phase! (10 iterations), m = 110\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 105\n",
      "Started ML phase! (10 iterations), m = 105\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 100\n",
      "Started ML phase! (10 iterations), m = 100\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 95\n",
      "Started ML phase! (10 iterations), m = 95\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 90\n",
      "Started ML phase! (10 iterations), m = 90\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 85\n",
      "Started ML phase! (10 iterations), m = 85\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 80\n",
      "Started ML phase! (10 iterations), m = 80\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 75\n",
      "Started ML phase! (10 iterations), m = 75\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 70\n",
      "Started ML phase! (10 iterations), m = 70\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 65\n",
      "Started ML phase! (10 iterations), m = 65\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 60\n",
      "Started ML phase! (10 iterations), m = 60\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 55\n",
      "Started ML phase! (10 iterations), m = 55\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "vif_k: 50\n",
      "Started ML phase! (10 iterations), m = 50\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#random.seed(110)\n",
    "\n",
    "\n",
    "res6 = pd.DataFrame({\"m\": {}, \"model_name\": {},\"variance_k\":{}, \"normality_k\":{}, \"boruta_flag\": {}, \"vif_k\":{}, \"score\": {}, \"chosen_columns\": {}, \"standardize\": {}})\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "for vif_k in range(260, 49, -5):\n",
    "    print(\"vif_k: \" + str(vif_k) )\n",
    "    \n",
    "    data_train, data_val = leaveKusingVIF_QUICK(data_train, data_val, vif_k)\n",
    "    \n",
    "    chosen_columns = str(data_train.columns.values)[1:-1]\n",
    "    best_cols = data_train.columns.values\n",
    "    \n",
    "    m = data_train.shape[1]\n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    scoress = []\n",
    "    \n",
    "    print(\"Started ML phase! (10 iterations), m = \" + str(m))\n",
    "\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "        data_train_, data_val_, y1_train, y1_val =  train_test_split(data_train, yd_train,train_size = 0.8)\n",
    "\n",
    "\n",
    "        # STANDARDIZING\n",
    "\n",
    "        scaler1.fit(data_train_)\n",
    "        columns1 = data_train_.columns\n",
    "        data_trains = pd.DataFrame(scaler1.transform(data_train_))\n",
    "        data_vals = pd.DataFrame(scaler1.transform(data_val_))\n",
    "        data_trains.columns = columns1\n",
    "        data_vals.columns = columns1   \n",
    "\n",
    "\n",
    "\n",
    "        # getting best model from feature selection previous step\n",
    "        xgboost = GradientBoostingClassifier(learning_rate = 0.2, n_estimators = 40, max_depth = 5)\n",
    "        xgboost.fit(data_train_, y1_train)\n",
    "        predictions = xgboost.predict(data_val_)\n",
    "        score = score2(predictions, y1_val, m)\n",
    "        scores.append(score)\n",
    "\n",
    "\n",
    "        xgboost.fit(data_trains, y1_train)\n",
    "        predictions = xgboost.predict(data_vals)\n",
    "        score = score2(predictions, y1_val, m)\n",
    "        scoress.append(score)\n",
    "\n",
    "    res6 = res6.append({\"m\": m,\"model_name\": \"xgb\", \"variance_k\": 500, \"normality_k\": 1, \"boruta_flag\": 1, \"vif_k\":vif_k, \"score\": np.mean(scoress), \"chosen_columns\": chosen_columns, \"standardize\": True}, ignore_index = True)\n",
    "    res6 = res6.append({\"m\": m,\"model_name\": \"xgb\", \"variance_k\": 500, \"normality_k\": 1, \"boruta_flag\": 1, \"vif_k\":vif_k, \"score\": np.mean(scores), \"chosen_columns\": chosen_columns, \"standardize\": False}, ignore_index = True)\n",
    "\n",
    "\n",
    "res6.to_csv(\"results-digits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee4f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "res6.to_csv(\"results-digits.csv\")\n",
    "\n",
    "#res6 = pd.read_csv(\"results-digits.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8511d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>model_name</th>\n",
       "      <th>variance_k</th>\n",
       "      <th>normality_k</th>\n",
       "      <th>boruta_flag</th>\n",
       "      <th>vif_k</th>\n",
       "      <th>score</th>\n",
       "      <th>chosen_columns</th>\n",
       "      <th>standardize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>240.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.964337</td>\n",
       "      <td>2   14   25   34   58   67   76  101  106  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>240.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.963479</td>\n",
       "      <td>2   14   25   34   58   67   76  101  106  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>170.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.962646</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>170.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.962641</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>180.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.962625</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>225.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.962533</td>\n",
       "      <td>2   14   34   58   67   76  101  106  139  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>180.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.962452</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>125.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.961829</td>\n",
       "      <td>14   34   58   67   76  139  174  180  269  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>225.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.961708</td>\n",
       "      <td>2   14   34   58   67   76  101  106  139  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>210.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.961472</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        m model_name  variance_k  normality_k  boruta_flag  vif_k     score  \\\n",
       "9   240.0        xgb       500.0          1.0          1.0  240.0  0.964337   \n",
       "8   240.0        xgb       500.0          1.0          1.0  240.0  0.963479   \n",
       "36  170.0        xgb       500.0          1.0          1.0  170.0  0.962646   \n",
       "37  170.0        xgb       500.0          1.0          1.0  170.0  0.962641   \n",
       "33  180.0        xgb       500.0          1.0          1.0  180.0  0.962625   \n",
       "14  225.0        xgb       500.0          1.0          1.0  225.0  0.962533   \n",
       "32  180.0        xgb       500.0          1.0          1.0  180.0  0.962452   \n",
       "54  125.0        xgb       500.0          1.0          1.0  125.0  0.961829   \n",
       "15  225.0        xgb       500.0          1.0          1.0  225.0  0.961708   \n",
       "20  210.0        xgb       500.0          1.0          1.0  210.0  0.961472   \n",
       "\n",
       "                                       chosen_columns  standardize  \n",
       "9      2   14   25   34   58   67   76  101  106  ...          0.0  \n",
       "8      2   14   25   34   58   67   76  101  106  ...          1.0  \n",
       "36     2   14   34   58   67   76  101  139  174  ...          1.0  \n",
       "37     2   14   34   58   67   76  101  139  174  ...          0.0  \n",
       "33     2   14   34   58   67   76  101  139  174  ...          0.0  \n",
       "14     2   14   34   58   67   76  101  106  139  ...          1.0  \n",
       "32     2   14   34   58   67   76  101  139  174  ...          1.0  \n",
       "54    14   34   58   67   76  139  174  180  269  ...          1.0  \n",
       "15     2   14   34   58   67   76  101  106  139  ...          0.0  \n",
       "20     2   14   34   58   67   76  101  139  174  ...          1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res6.sort_values(\"score\", ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c40bc",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning - digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "e6e20dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search:\n",
    "\n",
    "learning_rates = [0.1, 0.3, 0.5]\n",
    "n_ests = [30,40,50,60,70]\n",
    "max_depths = [5,6,7]\n",
    "\n",
    "ht_cases = np.array(np.meshgrid(learning_rates, n_ests, max_depths)).T.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "10ba65a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ht_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "8e93f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "\n",
    "splitted_columns = res6.loc[:,\"chosen_columns\"].map(lambda x: x.split(\" \"))\n",
    "\n",
    "for sc in splitted_columns:\n",
    "    aux = np.append(aux, sc)\n",
    "    \n",
    "aux2 = np.unique(aux, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "80d3b6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4779', '3051', '58', '14', '67', '180', '364', '4753', '2098',\n",
       "       '1695', '3034', '2924', '2977', '2808', '3746', '3670', '3845',\n",
       "       '2764', '493', '1870', '4301', '3550', '757', '3520', '1796',\n",
       "       '1500', '1793', '2149', '4105', '3196', '2856', '2783', '2781',\n",
       "       '1032', '3379', '4188', '1523', '4232', '76', '424', '1591', '278',\n",
       "       '678', '3355', '487', '3097', '1618', '4672', '3725', '1922',\n",
       "       '3638', '3897', '3774', '649', '2805', '1061', '1027', '558',\n",
       "       '865', '3609', '269', '445', '2330', '3782', '2835', '1073', '576',\n",
       "       '2801', '1296', '1716', '4615', '2982', '288', '4464', '1479',\n",
       "       '1471', '1108', '1580', '3846', '174', '2051', '934', '2485',\n",
       "       '4572', '2660', '2490', '2077', '3374', '34', '1983', '1654',\n",
       "       '3847', '4651', '3700', '4315', '616', '2367', '2782', '1099',\n",
       "       '2557', '4020', '1282', '2926', '4588', '2141', '1934', '139',\n",
       "       '2048', '4791', '2057', '1377', '4573', '1495', '3650', '315',\n",
       "       '2945', '4917', '467', '3144', '592', '295', '305', '1588', '4632',\n",
       "       '439', '4114', '3468', '985', '1663', '1680', '2068', '101',\n",
       "       '4979', '1257', '699', '1857', '2814', '803', '2', '2132', '1359',\n",
       "       '1966', '2695', '3506', '3514', '3433', '3642', '399', '2169',\n",
       "       '2244', '2895', '2031', '3647', '3238', '3233', '2904', '1006',\n",
       "       '4579', '2937', '3011', '2673', '2040', '4885', '3086', '395',\n",
       "       '2101', '1236', '2662', '3959', '2333', '2641', '2812', '1821',\n",
       "       '1762', '205', '2571', '579', '2598', '4129', '3385', '512',\n",
       "       '4390', '2430', '4493', '3844', '3185', '3543', '746', '4925',\n",
       "       '1258', '668', '1652', '4622', '1293', '3432', '2426', '328',\n",
       "       '537', '4553', '1376', '1813', '3188', '4271', '3995', '4869',\n",
       "       '106', '2510', '2621', '1400', '2733', '3923', '4694', '4856',\n",
       "       '1328', '3470', '4652', '1204', '2189', '1619', '1575', '4108',\n",
       "       '1600', '4794', '3313', '3269', '295\\n', '2769', '1099\\n',\n",
       "       '2801\\n', '2381', '4398', '3633', '288\\n', '3609\\n', '25', '455',\n",
       "       '2982\\n', '2032', '4863', '2490\\n', '2835\\n', '532', '269\\n',\n",
       "       '558\\n', '3847\\n', '3725\\n', '3418', '3774\\n', '3817', '4596',\n",
       "       '554', '568', '4573\\n', '1372', '1523\\n', '1006\\n', '803\\n',\n",
       "       '1471\\n', '467\\n', '4672\\n', '678\\n', '3827', '649\\n', '487\\n',\n",
       "       '1591\\n', '1618\\n', '4651\\n', '1296\\n', '3642\\n', '4232\\n', '3046',\n",
       "       '1966\\n', '3065', '1983\\n', '2923', '3355\\n', '278\\n', '3379\\n',\n",
       "       '3468\\n', '3097\\n', '2330\\n', '3514\\n', '668\\n', '4622\\n',\n",
       "       '2895\\n', '4579\\n', '2149\\n', '4546', '4464\\n', '1929', '1922\\n',\n",
       "       '1500\\n', '4188\\n', '865\\n', '3086\\n', '4020\\n', '2031\\n', '767',\n",
       "       '2856\\n', '2814\\n', '2101\\n', '1061\\n', '616\\n', '576\\n', '2660\\n',\n",
       "       '2673\\n', '1032\\n', '2068\\n', '688', '2781\\n', '2057\\n', '1580\\n',\n",
       "       '4694\\n', '2805\\n', '4876', '4794\\n', '2169\\n', '1654\\n', '3196\\n',\n",
       "       '3365', '3700\\n', '3782\\n', '315\\n', '1796\\n', '1793\\n', '3638\\n',\n",
       "       '1762\\n', '2764\\n', '4885\\n', '2051\\n', '2783\\n', '2048\\n',\n",
       "       '1257\\n', '4917\\n', '3995\\n', '1282\\n', '1293\\n', '3238\\n',\n",
       "       '4791\\n', '493\\n', '1258\\n', '3845\\n', '948', '934\\n', '2244\\n',\n",
       "       '3520\\n', '2333\\n', '1680\\n', '757\\n', '2367\\n', '3433\\n',\n",
       "       '2485\\n', '1663\\n', '3650\\n', '1108\\n', '3385\\n', '1118', '2641\\n',\n",
       "       '537\\n', '512\\n', '2904\\n', '3550\\n', '1870\\n', '4315\\n', '1479\\n',\n",
       "       '981', '4271\\n', '424\\n', '4412', '1495\\n', '305\\n', '4301\\n',\n",
       "       '1588\\n', '3011\\n', '1813\\n', '455\\n', '3046\\n', '4572\\n', '396',\n",
       "       '1857\\n', '4105\\n', '2926\\n', '2945\\n', '2621\\n', '2598\\n',\n",
       "       '2571\\n', '3188\\n', '2557\\n', '1073\\n', '1695\\n', '4129\\n',\n",
       "       '1027\\n', '3923\\n', '4195\\n', '2132\\n', '364\\n', '2381\\n',\n",
       "       '3185\\n', '1745', '4108\\n', '1550', '699\\n', '2141\\n', '2343',\n",
       "       '3959\\n', '2924\\n', '2098\\n', '2662\\n', '3233\\n', '3846\\n',\n",
       "       '4615\\n', '3844\\n', '1716\\n', '2032\\n', '3365\\n', '2812\\n',\n",
       "       '2977\\n', '2808\\n', '1934\\n', '4753\\n', '4493\\n', '1376\\n',\n",
       "       '3374\\n', '445\\n', '4856\\n', '1400\\n', '4585', '2695\\n', '1204\\n',\n",
       "       '4925\\n', '2077\\n', '3670\\n', '4390\\n', '4398\\n', '1652\\n',\n",
       "       '3897\\n', '3746\\n', '1619\\n', '2782\\n', '3034\\n', '3144\\n'],\n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, count = aux2\n",
    "\n",
    "count_sort_ind = np.argsort(-count)\n",
    "\n",
    "best_columns = u[count_sort_ind][1:]\n",
    "\n",
    "best_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "dce39437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>model_name</th>\n",
       "      <th>variance_k</th>\n",
       "      <th>normality_k</th>\n",
       "      <th>boruta_flag</th>\n",
       "      <th>vif_k</th>\n",
       "      <th>score</th>\n",
       "      <th>chosen_columns</th>\n",
       "      <th>standardize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>240.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.964337</td>\n",
       "      <td>2   14   25   34   58   67   76  101  106  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>240.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.963479</td>\n",
       "      <td>2   14   25   34   58   67   76  101  106  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>170.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.962646</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>170.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.962641</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>180.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.962625</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>225.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.962533</td>\n",
       "      <td>2   14   34   58   67   76  101  106  139  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>180.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.962452</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>125.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.961829</td>\n",
       "      <td>14   34   58   67   76  139  174  180  269  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>225.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.961708</td>\n",
       "      <td>2   14   34   58   67   76  101  106  139  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>210.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.961472</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>215.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.961227</td>\n",
       "      <td>2   14   34   58   67   76  101  106  139  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>230.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.961058</td>\n",
       "      <td>2   14   34   58   67   76  101  106  139  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>210.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.961028</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>125.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.960919</td>\n",
       "      <td>14   34   58   67   76  139  174  180  269  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>235.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.960893</td>\n",
       "      <td>2   14   25   34   58   67   76  101  106  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>165.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.960864</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>235.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.960796</td>\n",
       "      <td>2   14   25   34   58   67   76  101  106  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>195.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.960782</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>185.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.960748</td>\n",
       "      <td>2   14   34   58   67   76  101  139  174  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255.0</td>\n",
       "      <td>xgb</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.960732</td>\n",
       "      <td>2   14   25   34   58   67   76  101  106  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        m model_name  variance_k  normality_k  boruta_flag  vif_k     score  \\\n",
       "9   240.0        xgb       500.0          1.0          1.0  240.0  0.964337   \n",
       "8   240.0        xgb       500.0          1.0          1.0  240.0  0.963479   \n",
       "36  170.0        xgb       500.0          1.0          1.0  170.0  0.962646   \n",
       "37  170.0        xgb       500.0          1.0          1.0  170.0  0.962641   \n",
       "33  180.0        xgb       500.0          1.0          1.0  180.0  0.962625   \n",
       "14  225.0        xgb       500.0          1.0          1.0  225.0  0.962533   \n",
       "32  180.0        xgb       500.0          1.0          1.0  180.0  0.962452   \n",
       "54  125.0        xgb       500.0          1.0          1.0  125.0  0.961829   \n",
       "15  225.0        xgb       500.0          1.0          1.0  225.0  0.961708   \n",
       "20  210.0        xgb       500.0          1.0          1.0  210.0  0.961472   \n",
       "19  215.0        xgb       500.0          1.0          1.0  215.0  0.961227   \n",
       "13  230.0        xgb       500.0          1.0          1.0  230.0  0.961058   \n",
       "21  210.0        xgb       500.0          1.0          1.0  210.0  0.961028   \n",
       "55  125.0        xgb       500.0          1.0          1.0  125.0  0.960919   \n",
       "10  235.0        xgb       500.0          1.0          1.0  235.0  0.960893   \n",
       "39  165.0        xgb       500.0          1.0          1.0  165.0  0.960864   \n",
       "11  235.0        xgb       500.0          1.0          1.0  235.0  0.960796   \n",
       "27  195.0        xgb       500.0          1.0          1.0  195.0  0.960782   \n",
       "30  185.0        xgb       500.0          1.0          1.0  185.0  0.960748   \n",
       "3   255.0        xgb       500.0          1.0          1.0  255.0  0.960732   \n",
       "\n",
       "                                       chosen_columns  standardize  \n",
       "9      2   14   25   34   58   67   76  101  106  ...          0.0  \n",
       "8      2   14   25   34   58   67   76  101  106  ...          1.0  \n",
       "36     2   14   34   58   67   76  101  139  174  ...          1.0  \n",
       "37     2   14   34   58   67   76  101  139  174  ...          0.0  \n",
       "33     2   14   34   58   67   76  101  139  174  ...          0.0  \n",
       "14     2   14   34   58   67   76  101  106  139  ...          1.0  \n",
       "32     2   14   34   58   67   76  101  139  174  ...          1.0  \n",
       "54    14   34   58   67   76  139  174  180  269  ...          1.0  \n",
       "15     2   14   34   58   67   76  101  106  139  ...          0.0  \n",
       "20     2   14   34   58   67   76  101  139  174  ...          1.0  \n",
       "19     2   14   34   58   67   76  101  106  139  ...          0.0  \n",
       "13     2   14   34   58   67   76  101  106  139  ...          0.0  \n",
       "21     2   14   34   58   67   76  101  139  174  ...          0.0  \n",
       "55    14   34   58   67   76  139  174  180  269  ...          0.0  \n",
       "10     2   14   25   34   58   67   76  101  106  ...          1.0  \n",
       "39     2   14   34   58   67   76  101  139  174  ...          0.0  \n",
       "11     2   14   25   34   58   67   76  101  106  ...          0.0  \n",
       "27     2   14   34   58   67   76  101  139  174  ...          0.0  \n",
       "30     2   14   34   58   67   76  101  139  174  ...          1.0  \n",
       "3      2   14   25   34   58   67   76  101  106  ...          0.0  "
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res6.sort_values(\"score\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "e999d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "case6 = np.unique(np.asarray(res6.sort_values(\"score\", ascending = False).head(20)[[\"chosen_columns\"]].iloc[0,0].replace(\"   \",\",\").replace(\"  \", \",\").replace(\"\\n\", \"\").replace(\" \", \",\")[1:].split(\",\")).astype(int))\n",
    "case7 = np.unique(np.asarray(res6.sort_values(\"score\", ascending = False).head(20)[[\"chosen_columns\"]].iloc[2,0].replace(\"   \",\",\").replace(\"  \", \",\").replace(\"\\n\", \"\").replace(\" \", \",\")[1:].split(\",\")).astype(int))\n",
    "case8 = np.unique(np.asarray(res6.sort_values(\"score\", ascending = False).head(20)[[\"chosen_columns\"]].iloc[4,0].replace(\"   \",\",\").replace(\"  \", \",\").replace(\"\\n\", \"\").replace(\" \", \",\")[1:].split(\",\")).astype(int))\n",
    "case9 = np.unique(np.asarray(res6.sort_values(\"score\", ascending = False).head(20)[[\"chosen_columns\"]].iloc[5,0].replace(\"   \",\",\").replace(\"  \", \",\").replace(\"\\n\", \"\").replace(\" \", \",\")[1:].split(\",\")).astype(int))\n",
    "case10 = np.unique(np.asarray(res6.sort_values(\"score\", ascending = False).head(20)[[\"chosen_columns\"]].iloc[7,0].replace(\"   \",\",\").replace(\"  \", \",\").replace(\"\\n\", \"\").replace(\" \", \",\")[1:].split(\",\")).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "09f2f523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "170\n",
      "180\n",
      "225\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "print(len(case6))\n",
    "print(len(case7))\n",
    "print(len(case8))\n",
    "print(len(case9))\n",
    "print(len(case10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "6f90cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_cases = {\"case1\": best_columns[0:50].astype(int),\n",
    "               \"case2\": best_columns[0:60].astype(int),\n",
    "               \"case3\": best_columns[0:70].astype(int),\n",
    "               \"case4\": best_columns[0:80].astype(int),\n",
    "               \"case6\": case6,\n",
    "               \"case7\": case7,\n",
    "               \"case8\": case8,\n",
    "               \"case9\": case9,\n",
    "               \"case10\": case10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "71c42756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4779, 3051,   58,   14,   67,  180,  364, 4753, 2098, 1695, 3034,\n",
       "       2924, 2977, 2808, 3746, 3670, 3845, 2764,  493, 1870, 4301, 3550,\n",
       "        757, 3520, 1796, 1500, 1793, 2149, 4105, 3196, 2856, 2783, 2781,\n",
       "       1032, 3379, 4188, 1523, 4232,   76,  424, 1591,  278,  678, 3355,\n",
       "        487, 3097, 1618, 4672, 3725, 1922])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_cases[\"case1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "bdbc6266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,   34,   58,   67,   76,  139,  174,  180,  269,  278,  288,\n",
       "        295,  305,  315,  364,  424,  445,  467,  487,  493,  558,  576,\n",
       "        616,  649,  678,  757,  803,  865,  934, 1027, 1032, 1061, 1073,\n",
       "       1099, 1108, 1282, 1296, 1377, 1471, 1479, 1495, 1500, 1523, 1580,\n",
       "       1588, 1591, 1618, 1654, 1695, 1716, 1793, 1796, 1870, 1922, 1934,\n",
       "       1983, 2048, 2051, 2057, 2068, 2077, 2098, 2141, 2149, 2330, 2367,\n",
       "       2485, 2490, 2557, 2660, 2764, 2781, 2782, 2783, 2801, 2805, 2808,\n",
       "       2835, 2856, 2924, 2926, 2945, 2977, 2982, 3034, 3051, 3097, 3144,\n",
       "       3196, 3355, 3374, 3379, 3468, 3520, 3550, 3609, 3638, 3650, 3670,\n",
       "       3700, 3725, 3746, 3774, 3782, 3845, 3846, 3847, 3897, 4020, 4105,\n",
       "       4188, 4232, 4301, 4315, 4464, 4572, 4573, 4588, 4615, 4651, 4672,\n",
       "       4753, 4779, 4791, 4917])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_cases[\"case10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_train = pd.read_csv(\"data/digits_train.data\", sep = \" \", header = None).loc[:, 0:4999]\n",
    "yd_train = pd.read_csv(\"data/digits_train.labels\", sep = \" \", header = None)\n",
    "Xd_test = pd.read_csv(\"data/digits_valid.data\", sep = \" \", header = None).loc[:, 0:4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "d211b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: case6\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 22.22222222222222 %\n",
      "PROGRESS: 44.44444444444444 %\n",
      "PROGRESS: 66.66666666666667 %\n",
      "PROGRESS: 88.88888888888889 %\n",
      "case: case7\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 22.22222222222222 %\n",
      "PROGRESS: 44.44444444444444 %\n",
      "PROGRESS: 66.66666666666667 %\n",
      "PROGRESS: 88.88888888888889 %\n",
      "case: case8\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 22.22222222222222 %\n",
      "PROGRESS: 44.44444444444444 %\n",
      "PROGRESS: 66.66666666666667 %\n",
      "PROGRESS: 88.88888888888889 %\n",
      "case: case9\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 22.22222222222222 %\n",
      "PROGRESS: 44.44444444444444 %\n",
      "PROGRESS: 66.66666666666667 %\n",
      "PROGRESS: 88.88888888888889 %\n",
      "case: case10\n",
      "PROGRESS: 0.0 %\n",
      "PROGRESS: 22.22222222222222 %\n",
      "PROGRESS: 44.44444444444444 %\n",
      "PROGRESS: 66.66666666666667 %\n",
      "PROGRESS: 88.88888888888889 %\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# random.seed(110)\n",
    "\n",
    "res6_2 = pd.DataFrame({\"lr\": {},\"n_est\":{}, \"max_depth\":{}, \"m\": {}, \"score\": {}, \"case\": {}, \"standardize\": {}})\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "for case_col, best_cols in column_cases.items():\n",
    "    print(\"case: \" + case_col)\n",
    "    \n",
    "    m = len(best_cols)\n",
    "\n",
    "    # STANDARDIZING\n",
    "    \n",
    "    #     scaler1.fit(data_train)\n",
    "    #     columns1 = data_train.columns\n",
    "    #     data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "    #     data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "    #     data_trains.columns = columns1\n",
    "    #     data_vals.columns = columns1    \n",
    "    \n",
    "    # hyperparam tuning\n",
    "    happy_counter = 0\n",
    "    for case in ht_cases:\n",
    "        \n",
    "        if happy_counter % 10 == 0:\n",
    "            print(\"PROGRESS: \" + str(100*happy_counter/len(ht_cases)) + \" %\")\n",
    "        happy_counter += 1\n",
    "\n",
    "        scores = []\n",
    "        scoress = []\n",
    "\n",
    "        for i in range(10):\n",
    "            \n",
    "            data_train, data_val, y1_train, y1_val =  train_test_split(Xd_train, yd_train,train_size = 0.8)\n",
    "\n",
    "            # pick m best columns\n",
    "            data_train = data_train.iloc[:, best_cols]\n",
    "            data_val = data_val.iloc[:, best_cols]\n",
    "            \n",
    "            # STANDARDIZING\n",
    "\n",
    "            scaler1.fit(data_train)\n",
    "            columns1 = data_train.columns\n",
    "            data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "            data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "            data_trains.columns = columns1\n",
    "            data_vals.columns = columns1   \n",
    "\n",
    "            \n",
    "\n",
    "            # getting best model from feature selection previous step\n",
    "            xgboost = GradientBoostingClassifier(learning_rate = case[0], n_estimators = case[1].astype(int), max_depth = case[2].astype(int))\n",
    "            xgboost.fit(data_train, y1_train)\n",
    "            predictions = xgboost.predict(data_val)\n",
    "            score = score2(predictions, y1_val, m)\n",
    "            scores.append(score)\n",
    "\n",
    "\n",
    "            xgboost.fit(data_trains, y1_train)\n",
    "            predictions = xgboost.predict(data_vals)\n",
    "            score = score2(predictions, y1_val, m)\n",
    "            scoress.append(score)\n",
    "            \n",
    "        res6_2 = res6_2.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": np.mean(scores), \"case\": case_col, \"standardize\": False}, ignore_index = True)\n",
    "        res6_2 = res6_2.append({\"lr\": case[0], \"n_est\": case[1].astype(int), \"max_depth\": case[2].astype(int), \"m\": m, \"score\": np.mean(scoress), \"case\": case_col, \"standardize\": True}, ignore_index = True)\n",
    "\n",
    "\n",
    "res6_2.to_csv(\"results2-digits.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "4109fd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>n_est</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>m</th>\n",
       "      <th>score</th>\n",
       "      <th>case</th>\n",
       "      <th>standardize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.968909</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>0.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.968569</td>\n",
       "      <td>case8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>0.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.968042</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.967635</td>\n",
       "      <td>case7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.967593</td>\n",
       "      <td>case8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.967586</td>\n",
       "      <td>case7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.967470</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.967338</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.967255</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.967223</td>\n",
       "      <td>case8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.967076</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.967057</td>\n",
       "      <td>case7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.966974</td>\n",
       "      <td>case6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>case7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.966751</td>\n",
       "      <td>case8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.966728</td>\n",
       "      <td>case6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.966702</td>\n",
       "      <td>case6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>0.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.966635</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.966566</td>\n",
       "      <td>case6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.966561</td>\n",
       "      <td>case8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr  n_est  max_depth      m     score   case  standardize\n",
       "564  0.3   60.0        5.0  180.0  0.968909  case8          0.0\n",
       "577  0.5   70.0        5.0  180.0  0.968569  case8          1.0\n",
       "576  0.5   70.0        5.0  180.0  0.968042  case8          0.0\n",
       "475  0.3   60.0        5.0  170.0  0.967635  case7          1.0\n",
       "565  0.3   60.0        5.0  180.0  0.967593  case8          1.0\n",
       "472  0.3   50.0        5.0  170.0  0.967586  case7          0.0\n",
       "574  0.5   60.0        5.0  180.0  0.967470  case8          0.0\n",
       "622  0.3   50.0        7.0  180.0  0.967338  case8          0.0\n",
       "626  0.3   70.0        7.0  180.0  0.967255  case8          0.0\n",
       "575  0.5   60.0        5.0  180.0  0.967223  case8          1.0\n",
       "634  0.5   60.0        7.0  180.0  0.967076  case8          0.0\n",
       "514  0.5   60.0        6.0  170.0  0.967057  case7          0.0\n",
       "444  0.3   60.0        7.0  240.0  0.966974  case6          0.0\n",
       "515  0.5   60.0        6.0  170.0  0.966797  case7          1.0\n",
       "623  0.3   50.0        7.0  180.0  0.966751  case8          1.0\n",
       "426  0.5   70.0        6.0  240.0  0.966728  case6          0.0\n",
       "416  0.3   70.0        6.0  240.0  0.966702  case6          0.0\n",
       "602  0.5   50.0        6.0  180.0  0.966635  case8          0.0\n",
       "413  0.3   50.0        6.0  240.0  0.966566  case6          1.0\n",
       "596  0.3   70.0        6.0  180.0  0.966561  case8          0.0"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res6_2.sort_values(\"score\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381847e9",
   "metadata": {},
   "source": [
    "# Fitting best model - artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "6b57589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([1000, 1000], dtype=int64))"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ya_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "58d6eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_columns_artificial = [10, 28, 48, 64, 105, 204, 338, 442, 453, 493]\n",
    "\n",
    "random.seed(12948124)\n",
    "\n",
    "while(True):\n",
    "    xgboost = GradientBoostingClassifier(learning_rate = 0.15, n_estimators = 70, max_depth = 7)\n",
    "\n",
    "\n",
    "    # pick m best columns\n",
    "    data_train = Xa_train.iloc[:, best_columns_artificial]\n",
    "    data_val = Xa_test.iloc[:, best_columns_artificial]\n",
    "\n",
    "    # STANDARDIZING\n",
    "\n",
    "    scaler1.fit(data_train)\n",
    "    columns1 = data_train.columns\n",
    "    data_trains = pd.DataFrame(scaler1.transform(data_train))\n",
    "    data_vals = pd.DataFrame(scaler1.transform(data_val))\n",
    "    data_trains.columns = columns1\n",
    "    data_vals.columns = columns1 \n",
    "\n",
    "\n",
    "    # getting best model from feature selection previous step\n",
    "    xgboost.fit(data_trains, ya_train)\n",
    "    final_pred1 = xgboost.predict(data_vals)\n",
    "    if(all(np.unique(final_pred1, return_counts=True)[1] == [300,300])):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "599e51f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([300, 300], dtype=int64))"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(final_pred1, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "3c11adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred1 = xgboost.predict(data_vals)\n",
    "final_pred1_for_class1 = xgboost.predict_proba(data_vals)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "db21e2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02550807 0.01352801 0.6400402  0.98950637 0.49279684 0.99193103\n",
      " 0.11369741 0.00662171 0.38739941 0.98273366 0.97322846 0.96906869\n",
      " 0.07652691 0.97503956 0.87012255 0.00961455 0.06520293 0.99274942\n",
      " 0.09173147 0.01633236 0.56159898 0.0074684  0.09038846 0.87708801\n",
      " 0.72365581 0.98818687 0.02245784 0.25943412 0.36996618 0.02203831\n",
      " 0.04248064 0.9087953  0.85480717 0.14535989 0.99242469 0.03540018\n",
      " 0.89898216 0.98224404 0.87281689 0.96780082 0.59825735 0.97813108\n",
      " 0.87081635 0.94505914 0.09359547 0.99407662 0.01329385 0.63792277\n",
      " 0.13561856 0.28127997 0.9713875  0.27015938 0.43475415 0.95376706\n",
      " 0.97490824 0.02993033 0.98827186 0.6117387  0.809299   0.01624065\n",
      " 0.96156504 0.02274247 0.60974324 0.78515598 0.361503   0.07183233\n",
      " 0.37293392 0.94116034 0.61714622 0.70525897 0.95444175 0.76057486\n",
      " 0.98288019 0.95801582 0.98882266 0.00690946 0.89502888 0.60113394\n",
      " 0.98305917 0.1713756  0.08697661 0.95608753 0.69845186 0.9961858\n",
      " 0.93946804 0.69038634 0.90664056 0.75423446 0.74239346 0.94658169\n",
      " 0.49478079 0.00391328 0.12620067 0.02806455 0.96917445 0.03153237\n",
      " 0.57734644 0.08954539 0.93468346 0.8554334  0.00598159 0.31646364\n",
      " 0.23268804 0.91860813 0.05498686 0.98333853 0.63589452 0.65535492\n",
      " 0.98646299 0.96975408 0.96718993 0.82301612 0.75278192 0.0217334\n",
      " 0.03570869 0.10045898 0.96492517 0.23528351 0.67278114 0.00497876\n",
      " 0.01353919 0.90327807 0.97779459 0.60578088 0.24382316 0.90611066\n",
      " 0.05085728 0.01157178 0.98085273 0.22174127 0.97523973 0.34166401\n",
      " 0.05284454 0.98617661 0.01021962 0.97588714 0.18441627 0.84342483\n",
      " 0.38277567 0.01272745 0.75788129 0.79884332 0.97601318 0.47647247\n",
      " 0.17049256 0.98492088 0.64234208 0.04511763 0.01493212 0.02652689\n",
      " 0.98415858 0.01358018 0.98312049 0.04261681 0.05633071 0.45896918\n",
      " 0.98469277 0.9654177  0.44155005 0.18407669 0.92195578 0.90553653\n",
      " 0.41470699 0.0499285  0.03389493 0.01242683 0.28437979 0.98784291\n",
      " 0.01503158 0.2231835  0.97700572 0.62982075 0.39370405 0.01711279\n",
      " 0.62636852 0.99148352 0.0363304  0.01419564 0.92094715 0.18089153\n",
      " 0.92874431 0.28993899 0.96715567 0.77207207 0.01889939 0.32771969\n",
      " 0.32446507 0.40716246 0.24742776 0.01847461 0.96446814 0.94376768\n",
      " 0.05616421 0.19581754 0.49749321 0.99448565 0.07351486 0.98399454\n",
      " 0.29886069 0.00841127 0.08378394 0.9575971  0.97325936 0.82300577\n",
      " 0.32122781 0.02424442 0.85275008 0.98603821 0.81274646 0.67281353\n",
      " 0.05211082 0.05332253 0.02674548 0.02694189 0.01328219 0.11067605\n",
      " 0.98099761 0.99280477 0.04226597 0.01677725 0.98737436 0.98370497\n",
      " 0.33006923 0.2129965  0.36703038 0.68727009 0.02799809 0.01989502\n",
      " 0.58759079 0.96979889 0.35339169 0.0573334  0.96116963 0.44862809\n",
      " 0.61167332 0.80005869 0.08061635 0.09960286 0.21179728 0.99299434\n",
      " 0.79117555 0.05399691 0.97736732 0.32339962 0.02680249 0.98750408\n",
      " 0.98530989 0.06597251 0.03380286 0.92587513 0.72519529 0.14660571\n",
      " 0.10106896 0.12891765 0.00978328 0.97883103 0.15850776 0.01290868\n",
      " 0.97041793 0.98379016 0.97717319 0.98584982 0.98080866 0.11131432\n",
      " 0.91293196 0.96194967 0.14078737 0.16939666 0.97019489 0.01586558\n",
      " 0.90438654 0.80881693 0.00710725 0.2947233  0.09293114 0.04804337\n",
      " 0.87681499 0.83691945 0.94228929 0.96995864 0.27097527 0.03035887\n",
      " 0.5024527  0.98129342 0.93779427 0.09476444 0.00575318 0.84098493\n",
      " 0.92057202 0.11516636 0.30205996 0.18047915 0.94372434 0.99041599\n",
      " 0.99447968 0.96699152 0.28341302 0.20686554 0.97912009 0.16413075\n",
      " 0.03032307 0.04402824 0.98488989 0.805636   0.36910524 0.9965611\n",
      " 0.11235537 0.34735096 0.96064288 0.02069035 0.91039456 0.88071391\n",
      " 0.29969876 0.49725698 0.98522726 0.08266447 0.11200678 0.01753036\n",
      " 0.95041982 0.65516509 0.01176021 0.05000938 0.60744001 0.9565683\n",
      " 0.1107016  0.03494026 0.01569826 0.21142431 0.88064148 0.0826996\n",
      " 0.9911093  0.96340441 0.90828228 0.92849125 0.0262455  0.88803787\n",
      " 0.02798459 0.00977801 0.14736292 0.84514243 0.15472472 0.03495787\n",
      " 0.97938036 0.02178491 0.09288196 0.03520721 0.05395147 0.48740776\n",
      " 0.98006173 0.67831951 0.0985166  0.90864413 0.34642455 0.01101363\n",
      " 0.03364375 0.02065586 0.98299417 0.39262983 0.87737778 0.01016641\n",
      " 0.02075663 0.02503317 0.98778475 0.45726793 0.98433919 0.36173083\n",
      " 0.1273775  0.28425859 0.97602337 0.99460674 0.13780141 0.03351948\n",
      " 0.81352693 0.03205734 0.79660927 0.97178378 0.90698839 0.96282368\n",
      " 0.0502799  0.99480974 0.02802427 0.76765682 0.95437625 0.98443636\n",
      " 0.62852906 0.97661488 0.05431132 0.87428818 0.05995389 0.32067823\n",
      " 0.88627608 0.11871904 0.97618782 0.87390653 0.96716032 0.96930918\n",
      " 0.95262175 0.78459325 0.04007831 0.09447294 0.77305116 0.46633062\n",
      " 0.08501001 0.81350936 0.99097827 0.02139741 0.05487261 0.96845733\n",
      " 0.97732373 0.20133581 0.02306356 0.98643293 0.96376943 0.04319068\n",
      " 0.07878422 0.73606642 0.91965478 0.10319186 0.88614098 0.97567911\n",
      " 0.313323   0.09124197 0.96435318 0.48217719 0.24849011 0.83449138\n",
      " 0.01289772 0.19113016 0.76498667 0.27847528 0.24859797 0.91001014\n",
      " 0.12180186 0.06546051 0.90563422 0.82333672 0.47795307 0.97277801\n",
      " 0.01379111 0.01002982 0.98854972 0.31589081 0.96450778 0.03176705\n",
      " 0.02582705 0.97709153 0.96915131 0.99034147 0.90827213 0.57632159\n",
      " 0.89509901 0.98573442 0.96234498 0.21883801 0.07969189 0.01095695\n",
      " 0.80460275 0.08245517 0.91218404 0.30245856 0.20456684 0.03398303\n",
      " 0.06672027 0.03616331 0.98211062 0.02715329 0.91511978 0.98163289\n",
      " 0.94336566 0.03469282 0.77160286 0.80874356 0.26707736 0.03227318\n",
      " 0.89821752 0.95664785 0.22153397 0.43641918 0.05758936 0.98245168\n",
      " 0.03008545 0.9916124  0.55851251 0.98728269 0.10197326 0.72707455\n",
      " 0.97059651 0.10250493 0.91716908 0.61614547 0.11742521 0.04201735\n",
      " 0.96897273 0.59140113 0.60548323 0.95235228 0.89881788 0.97758298\n",
      " 0.98930815 0.09895746 0.22291007 0.60834625 0.04645953 0.07788024\n",
      " 0.98091247 0.36883814 0.86713691 0.90660996 0.98927172 0.94478402\n",
      " 0.00746906 0.01875384 0.79108609 0.4477697  0.02037877 0.0188059\n",
      " 0.1634544  0.02061094 0.8935846  0.95652944 0.87225043 0.83594672\n",
      " 0.88167921 0.46726945 0.76640189 0.06424521 0.99505961 0.07521731\n",
      " 0.75313175 0.04852582 0.88704579 0.94200395 0.94660921 0.43113661\n",
      " 0.81645036 0.41619813 0.97789096 0.9964715  0.05376057 0.65024417\n",
      " 0.22996641 0.32751774 0.2247947  0.01661468 0.01612223 0.98833249\n",
      " 0.06122756 0.96762063 0.94625485 0.01118368 0.10765983 0.02160506\n",
      " 0.95672266 0.46782059 0.95576132 0.43013534 0.81166603 0.91827181\n",
      " 0.08353828 0.93188629 0.90910227 0.01571063 0.95930848 0.88669494\n",
      " 0.47840527 0.07294645 0.97930089 0.19381617 0.21873361 0.4033314\n",
      " 0.44709288 0.87053639 0.13012669 0.03893532 0.72621796 0.11539885\n",
      " 0.96561185 0.92414535 0.80656487 0.04632781 0.20444772 0.97215058\n",
      " 0.0222371  0.03078123 0.62253644 0.9944607  0.96235405 0.64933344\n",
      " 0.15940821 0.010825   0.0707016  0.98194556 0.05703302 0.04122725\n",
      " 0.3427157  0.99080045 0.29132469 0.9835135  0.89353729 0.01050901]\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "print(final_pred1_for_class1)\n",
    "print(len(final_pred1_for_class1))\n",
    "pd.DataFrame(final_pred1_for_class1).to_csv(\"PATWRO_artificial_prediction.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "45ac7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_columns_artificial).to_csv(\"PATWRO_artificial_features.txt\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "5db45592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_columns_artificial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31459b7a",
   "metadata": {},
   "source": [
    "# Fitting best model - digits - did not change anything in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "79691e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest()"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to make some improvements - Isolation forest\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "las = IsolationForest()\n",
    "las.fit(Xd_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "600f8dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 80, 122, 129, 177, 214, 229, 331, 384, 652, 687, 787], dtype=int64),)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las.fit(Xd_test)\n",
    "if_predictions = las.predict(Xd_test)\n",
    "\n",
    "\n",
    "\n",
    "np.where(if_predictions == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "ed9b5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.where(if_predictions == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c2a5d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "las.fit(Xd_train)\n",
    "if_predictions = las.predict(Xd_train)\n",
    "indexes_tr = np.where(if_predictions == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "d1a07b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 80, 122, 129, 177, 214, 229, 331, 384, 652, 687, 787], dtype=int64),)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "1c9f53ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   8,   46,   80,  214,  286,  295,  326,  439,  516,  544,  560,\n",
       "         581,  603,  715,  718,  921,  962, 1259, 1378, 1499, 1579, 1605,\n",
       "        1659, 1759, 1835, 1979, 2064, 2139, 2198, 2315, 2367, 2580, 2613,\n",
       "        2692, 2757, 2790, 2845, 2919, 2955, 2966, 3205, 3252, 3357, 3397,\n",
       "        3418, 3553, 3882, 3938, 3940, 4040, 4044, 4083, 4169, 4204, 4404,\n",
       "        4487, 4589, 4627, 4803, 4847, 5043, 5215, 5223, 5236, 5274, 5279,\n",
       "        5400, 5408, 5484, 5531, 5555, 5592, 5773, 5856, 5869, 5890, 5931,\n",
       "        5952], dtype=int64),)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "b76c596d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([63, 15], dtype=int64))"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(yd_train.iloc[indexes_tr], return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf20f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "75790ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = GradientBoostingClassifier(learning_rate = 0.3, n_estimators = 60, max_depth = 5)\n",
    "\n",
    "cc = column_cases[\"case8\"]\n",
    "\n",
    "# pick m best columns\n",
    "data_train = Xd_train.iloc[:, cc]\n",
    "data_test = Xd_test.iloc[:, cc]\n",
    "\n",
    "\n",
    "# getting best model from feature selection previous step\n",
    "xgboost.fit(data_train, yd_train)\n",
    "final_pred2 = xgboost.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "40fee316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([9, 2], dtype=int64))"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(final_pred2[indexes],return_counts = True) # distribution 9:2 seems ok, the sae was in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "c8a75f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_proba2 = xgboost.predict_proba(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "2c3f4265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.64523848e-01, 8.35476152e-01],\n",
       "       [1.77551751e-04, 9.99822448e-01],\n",
       "       [9.99635161e-01, 3.64838560e-04],\n",
       "       ...,\n",
       "       [3.25058829e-03, 9.96749412e-01],\n",
       "       [9.99294331e-01, 7.05669456e-04],\n",
       "       [9.99873286e-01, 1.26714048e-04]])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_proba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "aafcced7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, -1,  1,  1, -1, -1, -1, -1, -1,  1,  1, -1,  1, -1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1, -1, -1,  1, -1,  1,  1, -1, -1,  1, -1,  1, -1, -1,\n",
       "       -1, -1, -1,  1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1,  1,\n",
       "        1,  1, -1, -1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1,  1,  1,  1,\n",
       "        1, -1,  1, -1,  1,  1,  1, -1, -1,  1, -1,  1, -1, -1,  1, -1, -1,\n",
       "        1,  1,  1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1, -1, -1, -1,\n",
       "        1, -1,  1, -1,  1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  1, -1,  1,\n",
       "        1,  1, -1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1, -1,  1, -1,  1,\n",
       "        1,  1, -1,  1, -1,  1, -1,  1, -1,  1, -1, -1, -1,  1, -1,  1,  1,\n",
       "        1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1,  1, -1, -1, -1,  1,  1,\n",
       "        1,  1, -1,  1,  1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,\n",
       "       -1, -1,  1, -1,  1, -1,  1, -1,  1,  1, -1, -1,  1, -1,  1, -1,  1,\n",
       "        1, -1,  1, -1, -1,  1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1,\n",
       "       -1,  1,  1, -1, -1, -1, -1,  1,  1, -1,  1,  1,  1,  1,  1, -1,  1,\n",
       "       -1,  1,  1, -1,  1,  1, -1, -1,  1, -1,  1, -1,  1,  1,  1, -1, -1,\n",
       "        1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1, -1, -1, -1,  1, -1,\n",
       "        1, -1, -1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1, -1, -1,\n",
       "       -1,  1,  1,  1,  1,  1, -1, -1,  1,  1, -1, -1,  1, -1, -1, -1,  1,\n",
       "       -1,  1, -1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1, -1,  1,  1,\n",
       "        1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,\n",
       "        1,  1, -1,  1,  1,  1, -1,  1, -1, -1,  1, -1,  1, -1, -1,  1,  1,\n",
       "       -1, -1, -1,  1,  1, -1,  1,  1,  1, -1,  1, -1, -1, -1,  1, -1, -1,\n",
       "        1,  1, -1, -1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1,  1,\n",
       "       -1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1,\n",
       "       -1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1, -1,\n",
       "       -1,  1,  1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1,  1, -1, -1,  1,\n",
       "       -1, -1,  1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1,  1,  1,  1,\n",
       "        1, -1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1,  1, -1, -1,  1,  1,\n",
       "       -1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1, -1, -1, -1,  1, -1,  1,\n",
       "       -1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,\n",
       "        1,  1,  1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1, -1, -1, -1,  1, -1,  1,  1, -1,  1,  1,\n",
       "        1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1,  1, -1,  1, -1,\n",
       "        1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1, -1, -1,\n",
       "        1, -1,  1,  1, -1,  1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1,\n",
       "        1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1, -1,\n",
       "       -1, -1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1,  1, -1, -1, -1, -1,\n",
       "       -1,  1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1,\n",
       "       -1, -1, -1,  1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1,  1, -1,  1,\n",
       "       -1,  1, -1,  1,  1, -1,  1, -1,  1,  1,  1, -1,  1,  1, -1, -1, -1,\n",
       "       -1,  1, -1,  1, -1, -1,  1, -1, -1,  1,  1, -1, -1,  1,  1, -1, -1,\n",
       "        1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1, -1,  1,\n",
       "        1,  1,  1, -1, -1, -1,  1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1,\n",
       "        1, -1, -1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1,\n",
       "        1, -1,  1, -1,  1, -1, -1,  1,  1, -1,  1, -1,  1,  1, -1, -1, -1,\n",
       "       -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  1, -1,  1, -1,  1,  1, -1,\n",
       "        1, -1,  1, -1, -1, -1,  1,  1,  1, -1, -1, -1,  1, -1,  1,  1,  1,\n",
       "       -1, -1, -1,  1,  1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1, -1,\n",
       "        1,  1,  1, -1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1, -1,  1, -1,\n",
       "       -1, -1,  1, -1,  1, -1, -1,  1,  1,  1, -1,  1, -1, -1, -1,  1,  1,\n",
       "       -1,  1,  1,  1, -1, -1,  1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,\n",
       "        1,  1, -1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1, -1, -1, -1, -1,\n",
       "       -1, -1, -1,  1, -1, -1, -1,  1, -1,  1,  1,  1,  1, -1, -1, -1, -1,\n",
       "        1,  1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
       "       -1,  1,  1,  1, -1, -1,  1, -1,  1, -1,  1, -1,  1, -1,  1,  1,  1,\n",
       "        1, -1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1,\n",
       "       -1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1, -1, -1,\n",
       "        1,  1,  1, -1, -1,  1, -1,  1, -1, -1,  1,  1, -1, -1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "415aa64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred2_for_class1 = final_proba2[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "5415b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.35476152e-01, 9.99822448e-01, 3.64838560e-04, 9.98906915e-01,\n",
       "       9.99752767e-01, 1.33785299e-04, 1.86762268e-04, 9.92905630e-05,\n",
       "       3.12824418e-04, 4.25349261e-04, 9.92104818e-01, 9.97865414e-01,\n",
       "       5.26128589e-03, 9.99913060e-01, 1.66168012e-03, 9.97646707e-01,\n",
       "       9.99664162e-01, 9.99679394e-01, 9.72714327e-01, 9.99473807e-01,\n",
       "       2.24588827e-03, 9.99211666e-01, 9.98461977e-01, 9.99643817e-01,\n",
       "       9.97227692e-01, 7.11015498e-03, 9.99826743e-01, 9.99663178e-01,\n",
       "       1.56720961e-02, 9.90996065e-01, 9.85119818e-01, 9.31977655e-01,\n",
       "       1.25335216e-02, 9.99842750e-01, 9.55549589e-01, 9.99845725e-01,\n",
       "       6.02265703e-01, 9.91710827e-01, 1.83752447e-02, 4.68202699e-04,\n",
       "       9.99730023e-01, 6.62749444e-04, 9.82229934e-01, 9.99657717e-01,\n",
       "       4.59192173e-04, 1.62844892e-04, 9.83932568e-01, 2.62438988e-02,\n",
       "       9.68891741e-01, 1.09502117e-03, 5.52819533e-04, 5.78399520e-04,\n",
       "       1.75258609e-04, 3.25616256e-04, 9.99065179e-01, 9.99929365e-01,\n",
       "       8.89318998e-01, 5.60822900e-04, 8.27673005e-01, 9.97881045e-01,\n",
       "       9.99811706e-01, 8.29461049e-05, 7.04434162e-05, 9.96167033e-01,\n",
       "       1.66522834e-03, 1.05367123e-03, 9.99956563e-01, 9.98885957e-01,\n",
       "       9.99753644e-01, 9.94988773e-01, 5.85803105e-04, 7.55077059e-03,\n",
       "       2.52638205e-02, 9.98060725e-01, 9.82054022e-01, 9.97669503e-01,\n",
       "       9.05615811e-04, 5.44694831e-04, 9.99131172e-01, 9.99105337e-01,\n",
       "       3.40485902e-04, 9.99630613e-01, 9.78105555e-01, 9.96130761e-01,\n",
       "       6.71518947e-01, 9.97439394e-01, 7.52287529e-04, 9.99630309e-01,\n",
       "       6.26933320e-03, 9.99346719e-01, 9.98872932e-01, 9.70944061e-01,\n",
       "       6.00301124e-04, 4.62135167e-04, 9.97120427e-01, 6.47896865e-03,\n",
       "       9.97427143e-01, 1.42437250e-03, 2.71330221e-01, 9.92401436e-01,\n",
       "       1.65085034e-03, 1.15755560e-03, 9.96078401e-01, 9.99814586e-01,\n",
       "       9.99579255e-01, 9.95911773e-01, 1.46437269e-04, 9.96229358e-01,\n",
       "       9.99783913e-01, 1.71187354e-04, 9.39421885e-01, 2.18759711e-03,\n",
       "       9.80212635e-01, 9.79650188e-03, 9.99362655e-01, 9.99807489e-01,\n",
       "       2.70734944e-04, 5.25990908e-05, 7.43975800e-04, 9.93154178e-01,\n",
       "       1.36273417e-03, 9.99560580e-01, 1.35033536e-02, 8.39497649e-01,\n",
       "       1.30078674e-03, 3.13193370e-02, 9.99703754e-01, 7.60745765e-03,\n",
       "       9.41297688e-04, 9.31861203e-04, 3.14516121e-03, 8.08135653e-02,\n",
       "       6.12174313e-04, 9.51026997e-01, 2.11859160e-04, 9.95843649e-01,\n",
       "       9.20265925e-01, 9.99867399e-01, 3.86195746e-04, 9.96300419e-01,\n",
       "       9.99580732e-01, 9.96490207e-01, 4.38111004e-04, 9.99741741e-01,\n",
       "       9.53633809e-01, 9.99611733e-01, 1.52324622e-04, 9.99923130e-01,\n",
       "       9.99780108e-01, 7.25632563e-04, 9.84797992e-01, 1.67561716e-04,\n",
       "       9.99752306e-01, 9.98160468e-01, 9.98887888e-01, 2.19482990e-03,\n",
       "       9.89554189e-01, 3.30023151e-04, 9.98683976e-01, 3.14667745e-02,\n",
       "       9.95706221e-01, 5.93655456e-04, 9.99278879e-01, 3.44428913e-03,\n",
       "       1.13270295e-01, 8.56544164e-04, 6.32396368e-01, 2.44276413e-04,\n",
       "       9.99643073e-01, 9.99858856e-01, 9.28179755e-01, 9.99337909e-01,\n",
       "       6.35323813e-04, 6.59919577e-04, 9.98904859e-01, 9.98780147e-01,\n",
       "       9.97391731e-01, 7.39325526e-01, 9.98824749e-01, 1.16879342e-02,\n",
       "       9.93937774e-05, 9.93107073e-01, 2.04073615e-03, 1.26293490e-03,\n",
       "       9.39925460e-04, 9.98517993e-01, 9.43446063e-01, 9.99295065e-01,\n",
       "       9.98916686e-01, 4.45251866e-02, 9.99851558e-01, 9.94607102e-01,\n",
       "       9.99628101e-01, 1.79755427e-04, 9.86402590e-01, 7.79397438e-04,\n",
       "       2.46539143e-03, 5.06186758e-04, 9.93727944e-01, 9.99872068e-01,\n",
       "       9.98954305e-01, 9.98448847e-01, 7.82096331e-04, 9.99123819e-01,\n",
       "       3.05832312e-03, 7.71055730e-04, 9.99268006e-01, 4.46669120e-04,\n",
       "       9.98928282e-01, 3.79478335e-02, 9.99274174e-01, 1.28050608e-02,\n",
       "       9.99587137e-01, 9.99682676e-01, 1.71694989e-04, 7.54570009e-03,\n",
       "       9.99704339e-01, 2.38448395e-04, 9.97425047e-01, 2.53011405e-02,\n",
       "       9.99713909e-01, 9.38118387e-01, 2.52645343e-04, 9.99531073e-01,\n",
       "       4.95142955e-04, 1.01511842e-02, 9.99737920e-01, 9.57664926e-01,\n",
       "       1.77741581e-02, 1.14730664e-02, 1.98874198e-04, 1.87904725e-04,\n",
       "       9.96498014e-01, 4.54494571e-05, 9.23338624e-05, 4.06829828e-04,\n",
       "       5.90616066e-04, 9.98352908e-01, 9.85935394e-04, 9.95798156e-01,\n",
       "       9.94403835e-01, 2.54386877e-01, 1.22027934e-03, 2.13478221e-04,\n",
       "       1.01931080e-04, 9.20353104e-01, 9.97719506e-01, 2.51329679e-01,\n",
       "       9.99837454e-01, 9.99686699e-01, 9.95893821e-01, 9.98398272e-01,\n",
       "       9.97036643e-01, 7.94122351e-04, 9.99306780e-01, 1.64479398e-03,\n",
       "       9.97925903e-01, 8.37048134e-01, 5.75956557e-03, 9.96630552e-01,\n",
       "       9.99920532e-01, 1.29231040e-04, 4.14318531e-03, 9.32726719e-01,\n",
       "       1.11833349e-02, 9.99097140e-01, 9.80813386e-04, 9.98668057e-01,\n",
       "       9.96715283e-01, 5.68901546e-01, 5.09120585e-03, 6.41003009e-04,\n",
       "       9.74447564e-01, 9.97803665e-01, 6.73800606e-04, 2.43905365e-04,\n",
       "       3.20387973e-03, 9.98072661e-01, 9.99649408e-01, 9.99883578e-01,\n",
       "       9.98010914e-01, 9.99142416e-01, 1.18805775e-04, 9.99422185e-01,\n",
       "       3.61781535e-04, 1.43913079e-03, 2.02335500e-02, 9.98145035e-01,\n",
       "       2.28246792e-04, 9.94812080e-01, 1.38894223e-04, 4.41100547e-04,\n",
       "       2.52974846e-01, 9.99572247e-01, 8.05565819e-04, 9.99710327e-01,\n",
       "       9.09603115e-01, 5.20430865e-01, 9.99013610e-01, 1.30134445e-03,\n",
       "       9.99950826e-01, 9.99624570e-01, 3.21466237e-03, 1.98103130e-03,\n",
       "       3.61547255e-04, 7.04188390e-04, 3.54045361e-03, 8.74610838e-01,\n",
       "       8.55016953e-01, 9.93914625e-01, 9.99937986e-01, 9.99788265e-01,\n",
       "       4.12898814e-04, 1.90936270e-04, 6.50046961e-01, 9.97616338e-01,\n",
       "       8.66470910e-05, 4.73639341e-04, 9.99676002e-01, 4.79869540e-02,\n",
       "       5.85786059e-05, 1.01715621e-03, 9.99791959e-01, 2.32547248e-04,\n",
       "       9.97846793e-01, 4.67182649e-04, 9.98577883e-01, 9.89623169e-01,\n",
       "       3.22722905e-03, 7.19891147e-03, 9.98322938e-01, 1.74690140e-04,\n",
       "       6.40442609e-03, 9.81582371e-01, 1.00578635e-03, 9.99654027e-01,\n",
       "       9.99700809e-01, 2.16468905e-02, 9.73786408e-01, 9.98997991e-01,\n",
       "       9.99492634e-01, 9.98487269e-01, 2.61999677e-02, 9.26859322e-03,\n",
       "       1.88139840e-04, 2.16625354e-01, 9.98664125e-01, 9.94631287e-01,\n",
       "       9.99497458e-01, 3.32095395e-03, 8.65847426e-01, 3.93662823e-02,\n",
       "       9.97414085e-01, 5.98726645e-04, 3.20423905e-04, 2.08881234e-02,\n",
       "       9.99753506e-01, 9.99699696e-01, 9.93765965e-01, 3.79310863e-01,\n",
       "       9.99723032e-01, 9.99801101e-01, 9.99531973e-01, 5.24791491e-02,\n",
       "       9.98288868e-01, 9.81190446e-04, 8.95960714e-03, 9.99743739e-01,\n",
       "       4.13197691e-02, 9.95692180e-01, 2.41601897e-03, 1.68765500e-04,\n",
       "       9.65912717e-01, 7.13813604e-01, 2.40871252e-03, 3.09144647e-04,\n",
       "       5.73120855e-04, 9.99846459e-01, 9.98726322e-01, 3.29924811e-04,\n",
       "       8.33816561e-01, 9.99433080e-01, 9.99249674e-01, 9.08686177e-02,\n",
       "       9.98808096e-01, 6.15110676e-04, 2.34606140e-04, 3.40527686e-01,\n",
       "       9.84211561e-01, 7.73824920e-03, 1.01161612e-03, 9.97982004e-01,\n",
       "       9.83571219e-01, 1.14738116e-03, 4.29640898e-03, 4.01473126e-03,\n",
       "       9.99180031e-01, 1.56827748e-03, 8.64695922e-01, 9.98657524e-01,\n",
       "       9.98151816e-01, 9.99812608e-01, 9.87821767e-05, 9.99235803e-01,\n",
       "       9.81765813e-01, 3.17280390e-02, 5.62216819e-03, 9.79781302e-01,\n",
       "       9.36845575e-03, 4.17955191e-02, 1.89923767e-04, 9.61513308e-01,\n",
       "       7.64299742e-04, 7.82767358e-04, 5.28047094e-01, 9.98987978e-01,\n",
       "       2.16740794e-01, 3.73648948e-04, 4.12219256e-04, 8.82183276e-05,\n",
       "       9.99488042e-01, 9.99898260e-01, 9.98857924e-01, 9.08425649e-04,\n",
       "       9.94440452e-01, 1.12560664e-03, 9.98896946e-01, 9.99722057e-01,\n",
       "       9.05521466e-04, 1.54200786e-02, 9.75682020e-01, 9.15277600e-04,\n",
       "       6.95287989e-04, 8.51555060e-01, 1.56160089e-04, 9.97508059e-01,\n",
       "       9.97738958e-01, 9.98940725e-01, 9.98926181e-01, 9.92828209e-01,\n",
       "       7.75267965e-03, 7.14777417e-03, 7.95505268e-03, 9.99668460e-01,\n",
       "       9.99579709e-01, 9.97131533e-01, 9.81337870e-01, 1.63295786e-04,\n",
       "       3.65643892e-02, 1.49986668e-04, 8.29589226e-04, 9.98205542e-01,\n",
       "       6.09050788e-04, 9.98949999e-01, 4.35228921e-02, 9.93903619e-01,\n",
       "       1.42936924e-02, 1.74546150e-04, 9.99648359e-01, 1.85584203e-04,\n",
       "       3.06889571e-04, 9.95478946e-01, 9.00793955e-02, 2.82535150e-03,\n",
       "       9.91575517e-01, 9.97882672e-01, 9.95215123e-01, 3.97974721e-04,\n",
       "       9.99529611e-01, 3.61466410e-04, 9.80921395e-01, 3.25700259e-04,\n",
       "       9.64631039e-04, 9.99189856e-01, 9.99298221e-01, 9.99781625e-01,\n",
       "       9.99683711e-01, 1.10019827e-03, 1.48797750e-03, 9.98575516e-01,\n",
       "       9.97954762e-01, 9.80923191e-01, 2.45469745e-03, 1.08871722e-03,\n",
       "       9.99737466e-01, 9.64294631e-01, 3.71807247e-03, 9.63460555e-01,\n",
       "       9.99530568e-01, 1.18363373e-04, 3.25225473e-04, 5.59918953e-01,\n",
       "       9.70990798e-01, 1.15536398e-03, 3.00725647e-04, 9.97654918e-01,\n",
       "       8.65860063e-01, 9.75764359e-01, 9.94779935e-01, 2.33534476e-03,\n",
       "       9.93073964e-01, 9.99470567e-01, 1.84367336e-04, 3.62878544e-04,\n",
       "       9.58218748e-02, 1.41602947e-04, 2.51875168e-03, 9.90820179e-01,\n",
       "       9.26317938e-02, 9.99843440e-01, 9.18963563e-03, 9.99571811e-01,\n",
       "       9.95534730e-01, 8.12192374e-04, 1.25862647e-03, 1.97972112e-03,\n",
       "       3.51152033e-03, 1.41765212e-03, 2.67861375e-04, 1.55173561e-03,\n",
       "       9.89834927e-02, 1.19267678e-04, 6.08181702e-01, 9.98761882e-01,\n",
       "       9.53051334e-01, 4.16198823e-04, 7.13850251e-03, 9.95302514e-01,\n",
       "       9.48778001e-01, 9.99104985e-01, 5.63222711e-04, 1.49482638e-03,\n",
       "       9.26899922e-01, 1.12214093e-01, 9.40175747e-01, 9.49471219e-01,\n",
       "       8.89480415e-04, 9.84780996e-01, 1.23678768e-04, 9.92821849e-01,\n",
       "       5.51842422e-06, 9.99856419e-01, 4.77895223e-04, 1.89913052e-02,\n",
       "       4.64457289e-03, 3.08524215e-05, 9.98412827e-01, 9.99471079e-01,\n",
       "       9.98811680e-01, 9.80195841e-01, 9.97959924e-01, 6.51132108e-04,\n",
       "       5.62635060e-04, 1.27302884e-03, 9.41618197e-01, 1.40273372e-02,\n",
       "       9.97995806e-01, 9.99683578e-01, 2.42298863e-03, 9.99786831e-01,\n",
       "       9.92389304e-01, 9.90080894e-01, 9.96736116e-01, 1.65699467e-04,\n",
       "       1.92266606e-04, 9.99181366e-01, 1.33066780e-03, 3.59520936e-04,\n",
       "       2.29721754e-03, 8.68480746e-04, 9.89865555e-01, 9.99592163e-01,\n",
       "       5.07675512e-04, 9.99614271e-01, 9.99730893e-01, 1.63454041e-01,\n",
       "       9.98058034e-01, 3.91378646e-01, 8.24767204e-01, 3.99225830e-04,\n",
       "       2.87640729e-05, 1.50845642e-04, 2.08327713e-03, 2.38884906e-02,\n",
       "       4.29967780e-01, 1.99306185e-04, 4.19113174e-04, 5.24225488e-04,\n",
       "       9.99588616e-01, 9.98364362e-01, 9.99525235e-01, 2.41324482e-02,\n",
       "       7.91720015e-02, 1.20066162e-03, 1.14161744e-03, 9.86114285e-01,\n",
       "       8.74471923e-05, 9.97992739e-01, 9.95464239e-01, 1.46813084e-05,\n",
       "       7.38617155e-01, 9.93158475e-01, 3.06468336e-04, 9.96058672e-01,\n",
       "       2.53845910e-04, 7.92721483e-04, 4.08260557e-04, 7.24604694e-03,\n",
       "       9.95156736e-01, 9.91902006e-01, 1.42155553e-02, 9.94139079e-01,\n",
       "       9.99670255e-01, 1.45974675e-04, 9.98929250e-01, 1.01287604e-01,\n",
       "       3.31324046e-04, 6.02508161e-04, 6.81200136e-04, 9.82800028e-01,\n",
       "       5.05269706e-03, 2.55806789e-03, 8.64821650e-03, 2.37022455e-02,\n",
       "       9.98118869e-01, 9.99349849e-01, 9.99036429e-01, 1.32069823e-04,\n",
       "       1.20770575e-04, 2.81368369e-04, 8.20664656e-05, 9.99024775e-01,\n",
       "       8.94144511e-01, 1.11925895e-03, 9.99648104e-01, 9.99737535e-01,\n",
       "       7.16598298e-04, 9.99648236e-01, 9.99937098e-01, 9.99058191e-01,\n",
       "       2.26525392e-03, 9.99688267e-01, 1.04676521e-02, 1.07512260e-01,\n",
       "       3.75214996e-01, 1.95211090e-04, 1.12246540e-04, 9.99473799e-01,\n",
       "       4.27038539e-03, 9.99047151e-01, 3.38628900e-04, 3.76406466e-03,\n",
       "       2.48915845e-03, 9.98950065e-01, 2.71351065e-04, 2.17461214e-04,\n",
       "       3.45868263e-03, 3.49943969e-04, 7.81482451e-05, 9.99740967e-01,\n",
       "       9.98171229e-01, 9.99773425e-01, 9.99266092e-01, 4.17291943e-02,\n",
       "       1.46776842e-03, 6.83287884e-04, 9.99318872e-01, 1.79141299e-03,\n",
       "       4.10444428e-03, 9.99876757e-01, 1.93333180e-03, 6.25872311e-04,\n",
       "       9.99771022e-01, 9.97632794e-01, 1.14480871e-03, 1.54707572e-03,\n",
       "       3.00767504e-03, 9.99265414e-01, 4.72611544e-03, 9.97474508e-01,\n",
       "       5.32437654e-02, 9.93454564e-01, 3.03185795e-02, 9.98155901e-01,\n",
       "       9.99237149e-01, 3.14437712e-04, 9.97636733e-01, 5.14719782e-03,\n",
       "       9.98648718e-01, 9.99674206e-01, 9.99641223e-01, 1.02411042e-04,\n",
       "       9.93979144e-01, 9.94281952e-01, 1.77702224e-04, 2.32538559e-03,\n",
       "       6.78063498e-03, 5.83282526e-04, 9.95440293e-01, 1.41384944e-03,\n",
       "       9.96756818e-01, 1.06907020e-03, 2.94098998e-03, 9.97334382e-01,\n",
       "       1.04197748e-01, 1.75440123e-03, 9.94423152e-01, 9.93497602e-01,\n",
       "       1.01343199e-02, 4.91076598e-04, 9.80177869e-01, 9.99821107e-01,\n",
       "       1.66019532e-02, 5.35721579e-04, 9.99430862e-01, 1.67055537e-03,\n",
       "       3.90781513e-04, 2.75240692e-04, 3.73325380e-02, 9.99602933e-01,\n",
       "       9.95384742e-01, 9.99769543e-01, 9.99158108e-01, 9.99489692e-01,\n",
       "       9.99631372e-01, 9.98646461e-01, 1.49996658e-04, 2.94459254e-02,\n",
       "       9.98694535e-01, 3.68839441e-03, 9.99303615e-01, 9.98849209e-01,\n",
       "       9.97779541e-01, 9.51219147e-01, 1.08870845e-04, 8.95658928e-03,\n",
       "       2.38667483e-03, 9.99683105e-01, 9.96553302e-01, 9.73854729e-01,\n",
       "       3.68006941e-04, 5.81123964e-04, 3.63475512e-04, 8.66847836e-04,\n",
       "       9.99945050e-01, 7.10062606e-04, 9.99430379e-01, 2.55804145e-04,\n",
       "       9.99139131e-01, 9.36314844e-04, 4.01262108e-03, 8.79121570e-01,\n",
       "       3.08357148e-04, 9.99717791e-01, 6.70546360e-04, 9.86175341e-01,\n",
       "       9.99814280e-01, 9.16613332e-04, 9.98047581e-01, 9.82278093e-01,\n",
       "       8.43506662e-01, 1.79314160e-02, 9.92942180e-01, 5.36263604e-03,\n",
       "       9.99658034e-01, 9.98770650e-01, 6.17785843e-03, 9.99559504e-01,\n",
       "       1.65284892e-04, 7.11188277e-01, 1.07494991e-04, 9.02183482e-04,\n",
       "       7.86111375e-01, 9.99810426e-01, 4.47618089e-05, 9.99816953e-01,\n",
       "       1.04549977e-04, 9.98892728e-01, 9.99701449e-01, 3.40487271e-04,\n",
       "       5.88619130e-04, 7.94522255e-02, 8.44721761e-03, 1.44433437e-03,\n",
       "       3.00767784e-04, 9.08957691e-01, 1.81482608e-04, 2.37960954e-04,\n",
       "       6.86670216e-04, 1.19060375e-03, 3.63735778e-04, 7.59688473e-05,\n",
       "       9.96918138e-01, 1.80969043e-05, 9.99814160e-01, 1.06382970e-03,\n",
       "       9.91647292e-01, 9.61915269e-01, 1.88335553e-04, 9.95577318e-01,\n",
       "       4.77571551e-03, 9.99260327e-01, 7.62983365e-04, 1.65797012e-02,\n",
       "       5.86551448e-05, 9.62174548e-01, 9.99862337e-01, 9.61757391e-01,\n",
       "       6.88067282e-03, 3.33053101e-02, 1.20019772e-04, 9.98491897e-01,\n",
       "       1.96180711e-01, 9.99683110e-01, 9.98531640e-01, 9.78135808e-01,\n",
       "       3.76771743e-05, 9.53285878e-05, 5.17817904e-05, 9.97765328e-01,\n",
       "       9.60627431e-01, 5.81401985e-01, 9.99752383e-01, 9.85160669e-01,\n",
       "       4.29062241e-03, 2.21493093e-04, 9.98842788e-01, 3.63213808e-04,\n",
       "       3.37331399e-02, 9.99753999e-01, 3.51573566e-04, 9.99318692e-01,\n",
       "       1.15207651e-04, 7.17953894e-01, 9.99911896e-01, 9.88794060e-01,\n",
       "       1.36051025e-04, 3.06769168e-04, 9.03718053e-05, 9.99158405e-01,\n",
       "       9.97406004e-01, 6.86760357e-03, 4.50727342e-04, 9.80949349e-01,\n",
       "       9.94987005e-01, 9.98117648e-01, 7.89600071e-04, 1.14760634e-04,\n",
       "       9.99256259e-01, 1.84876073e-03, 1.01992979e-03, 1.25186080e-03,\n",
       "       9.89079793e-01, 4.21799208e-04, 9.99350597e-01, 3.79847080e-05,\n",
       "       8.16218866e-04, 9.99376821e-01, 9.99094571e-01, 9.98672530e-01,\n",
       "       6.39006603e-04, 9.93377047e-01, 3.16250897e-03, 5.94996361e-03,\n",
       "       2.02455728e-04, 9.51531424e-01, 9.97853510e-01, 3.65695851e-04,\n",
       "       9.96225560e-01, 9.79979149e-01, 9.90545566e-01, 5.86259025e-04,\n",
       "       9.07365691e-03, 9.99280307e-01, 1.48732997e-03, 9.99630924e-01,\n",
       "       1.40464640e-03, 3.89770056e-03, 9.99939820e-01, 2.25009901e-01,\n",
       "       2.59799495e-04, 1.67397651e-03, 1.75444321e-03, 9.06965856e-01,\n",
       "       8.84442859e-01, 9.94468208e-01, 1.06758381e-03, 9.91895231e-01,\n",
       "       9.47824419e-01, 9.99621882e-01, 1.80545306e-03, 6.22157300e-01,\n",
       "       9.97723112e-01, 5.27868377e-01, 8.93214681e-02, 7.08491244e-01,\n",
       "       9.94435735e-01, 1.41843091e-02, 3.92755572e-02, 2.43086820e-04,\n",
       "       1.48863270e-04, 4.38916253e-04, 1.47285870e-03, 2.00956209e-02,\n",
       "       9.97226770e-01, 8.68884773e-02, 4.76127982e-05, 1.12107044e-03,\n",
       "       9.88774464e-01, 2.62365761e-01, 9.99515466e-01, 9.97064431e-01,\n",
       "       9.88144581e-01, 9.97244454e-01, 5.28897469e-04, 1.68985312e-04,\n",
       "       2.77219741e-02, 9.53699338e-05, 9.99879588e-01, 9.98237292e-01,\n",
       "       9.98865638e-01, 9.85329582e-01, 9.99735317e-01, 1.96405677e-04,\n",
       "       9.99667688e-01, 2.89571876e-02, 9.18734963e-01, 7.40207140e-05,\n",
       "       8.94154191e-04, 3.77007923e-04, 1.48616964e-04, 4.46476125e-04,\n",
       "       7.40084325e-04, 7.74229718e-04, 9.99539213e-01, 1.20688459e-03,\n",
       "       9.95585762e-01, 9.95775056e-01, 9.99842612e-01, 1.68772140e-01,\n",
       "       4.57599136e-03, 8.66271160e-01, 6.76926265e-04, 9.99893882e-01,\n",
       "       2.07606360e-04, 9.96222475e-01, 6.91901104e-03, 9.97347087e-01,\n",
       "       4.38040536e-02, 9.99508074e-01, 9.88357752e-01, 9.94619692e-01,\n",
       "       9.56856772e-01, 4.23179348e-04, 9.98218965e-01, 9.72690357e-01,\n",
       "       1.08645869e-03, 9.87623612e-01, 9.99817631e-01, 9.99299200e-01,\n",
       "       6.64130311e-03, 9.99676572e-01, 1.38243592e-04, 9.77510770e-01,\n",
       "       9.93573992e-01, 2.91773491e-03, 9.99690019e-01, 9.99774572e-01,\n",
       "       9.99912998e-01, 2.01760431e-03, 8.74400547e-02, 9.96306939e-01,\n",
       "       1.13983849e-03, 9.99082143e-01, 3.49852964e-04, 1.64099059e-03,\n",
       "       2.15901080e-03, 8.97402345e-01, 7.63484584e-01, 9.99334397e-01,\n",
       "       9.82223344e-01, 9.83431163e-01, 3.71148731e-02, 9.07333913e-01,\n",
       "       2.78423080e-03, 1.70931699e-03, 9.87638660e-01, 9.97405023e-01,\n",
       "       9.99812469e-01, 4.51666754e-03, 1.50963547e-04, 9.98871746e-01,\n",
       "       9.72164460e-03, 9.96337253e-01, 4.38790953e-01, 1.92969149e-03,\n",
       "       9.96633524e-01, 9.96749412e-01, 7.05669456e-04, 1.26714048e-04])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred2_for_class1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b96e3",
   "metadata": {},
   "source": [
    "Trying to assign -1 predictions to observations where probability ~0.5 so that there will be a balance between predicted classes. Occurences of both will be 300. It could be done with consecutively training models and counting the classes' occurences just as in case of artificial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "b54f369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predd = xgboost.predict_proba(data_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "6b2d0362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2567, 2724, 5903], dtype=int64),)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(abs(predd-0.5) <= 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "3afa905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([297, 303], dtype=int64))"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(yd_train.iloc[np.where(abs(predd-0.5) <= 0.28)],return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "6e7fc9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(final_pred2[np.where(abs(final_pred2_for_class1-0.5) <= 0.028)],return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "yd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9943541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "4bc9023b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([3000, 3000], dtype=int64))"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(xgboost.predict(data_train), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "c50b22d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1], dtype=int64), array([498, 502], dtype=int64))"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(xgboost.predict(data_test), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "93531ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred2_for_class1[np.where(abs(final_pred2_for_class1-0.5) <= 0.028)] = [0.4, 0.41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "9b9b02ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.35476152e-01, 9.99822448e-01, 3.64838560e-04, 9.98906915e-01,\n",
       "       9.99752767e-01, 1.33785299e-04, 1.86762268e-04, 9.92905630e-05,\n",
       "       3.12824418e-04, 4.25349261e-04, 9.92104818e-01, 9.97865414e-01,\n",
       "       5.26128589e-03, 9.99913060e-01, 1.66168012e-03, 9.97646707e-01,\n",
       "       9.99664162e-01, 9.99679394e-01, 9.72714327e-01, 9.99473807e-01,\n",
       "       2.24588827e-03, 9.99211666e-01, 9.98461977e-01, 9.99643817e-01,\n",
       "       9.97227692e-01, 7.11015498e-03, 9.99826743e-01, 9.99663178e-01,\n",
       "       1.56720961e-02, 9.90996065e-01, 9.85119818e-01, 9.31977655e-01,\n",
       "       1.25335216e-02, 9.99842750e-01, 9.55549589e-01, 9.99845725e-01,\n",
       "       6.02265703e-01, 9.91710827e-01, 1.83752447e-02, 4.68202699e-04,\n",
       "       9.99730023e-01, 6.62749444e-04, 9.82229934e-01, 9.99657717e-01,\n",
       "       4.59192173e-04, 1.62844892e-04, 9.83932568e-01, 2.62438988e-02,\n",
       "       9.68891741e-01, 1.09502117e-03, 5.52819533e-04, 5.78399520e-04,\n",
       "       1.75258609e-04, 3.25616256e-04, 9.99065179e-01, 9.99929365e-01,\n",
       "       8.89318998e-01, 5.60822900e-04, 8.27673005e-01, 9.97881045e-01,\n",
       "       9.99811706e-01, 8.29461049e-05, 7.04434162e-05, 9.96167033e-01,\n",
       "       1.66522834e-03, 1.05367123e-03, 9.99956563e-01, 9.98885957e-01,\n",
       "       9.99753644e-01, 9.94988773e-01, 5.85803105e-04, 7.55077059e-03,\n",
       "       2.52638205e-02, 9.98060725e-01, 9.82054022e-01, 9.97669503e-01,\n",
       "       9.05615811e-04, 5.44694831e-04, 9.99131172e-01, 9.99105337e-01,\n",
       "       3.40485902e-04, 9.99630613e-01, 9.78105555e-01, 9.96130761e-01,\n",
       "       6.71518947e-01, 9.97439394e-01, 7.52287529e-04, 9.99630309e-01,\n",
       "       6.26933320e-03, 9.99346719e-01, 9.98872932e-01, 9.70944061e-01,\n",
       "       6.00301124e-04, 4.62135167e-04, 9.97120427e-01, 6.47896865e-03,\n",
       "       9.97427143e-01, 1.42437250e-03, 2.71330221e-01, 9.92401436e-01,\n",
       "       1.65085034e-03, 1.15755560e-03, 9.96078401e-01, 9.99814586e-01,\n",
       "       9.99579255e-01, 9.95911773e-01, 1.46437269e-04, 9.96229358e-01,\n",
       "       9.99783913e-01, 1.71187354e-04, 9.39421885e-01, 2.18759711e-03,\n",
       "       9.80212635e-01, 9.79650188e-03, 9.99362655e-01, 9.99807489e-01,\n",
       "       2.70734944e-04, 5.25990908e-05, 7.43975800e-04, 9.93154178e-01,\n",
       "       1.36273417e-03, 9.99560580e-01, 1.35033536e-02, 8.39497649e-01,\n",
       "       1.30078674e-03, 3.13193370e-02, 9.99703754e-01, 7.60745765e-03,\n",
       "       9.41297688e-04, 9.31861203e-04, 3.14516121e-03, 8.08135653e-02,\n",
       "       6.12174313e-04, 9.51026997e-01, 2.11859160e-04, 9.95843649e-01,\n",
       "       9.20265925e-01, 9.99867399e-01, 3.86195746e-04, 9.96300419e-01,\n",
       "       9.99580732e-01, 9.96490207e-01, 4.38111004e-04, 9.99741741e-01,\n",
       "       9.53633809e-01, 9.99611733e-01, 1.52324622e-04, 9.99923130e-01,\n",
       "       9.99780108e-01, 7.25632563e-04, 9.84797992e-01, 1.67561716e-04,\n",
       "       9.99752306e-01, 9.98160468e-01, 9.98887888e-01, 2.19482990e-03,\n",
       "       9.89554189e-01, 3.30023151e-04, 9.98683976e-01, 3.14667745e-02,\n",
       "       9.95706221e-01, 5.93655456e-04, 9.99278879e-01, 3.44428913e-03,\n",
       "       1.13270295e-01, 8.56544164e-04, 6.32396368e-01, 2.44276413e-04,\n",
       "       9.99643073e-01, 9.99858856e-01, 9.28179755e-01, 9.99337909e-01,\n",
       "       6.35323813e-04, 6.59919577e-04, 9.98904859e-01, 9.98780147e-01,\n",
       "       9.97391731e-01, 7.39325526e-01, 9.98824749e-01, 1.16879342e-02,\n",
       "       9.93937774e-05, 9.93107073e-01, 2.04073615e-03, 1.26293490e-03,\n",
       "       9.39925460e-04, 9.98517993e-01, 9.43446063e-01, 9.99295065e-01,\n",
       "       9.98916686e-01, 4.45251866e-02, 9.99851558e-01, 9.94607102e-01,\n",
       "       9.99628101e-01, 1.79755427e-04, 9.86402590e-01, 7.79397438e-04,\n",
       "       2.46539143e-03, 5.06186758e-04, 9.93727944e-01, 9.99872068e-01,\n",
       "       9.98954305e-01, 9.98448847e-01, 7.82096331e-04, 9.99123819e-01,\n",
       "       3.05832312e-03, 7.71055730e-04, 9.99268006e-01, 4.46669120e-04,\n",
       "       9.98928282e-01, 3.79478335e-02, 9.99274174e-01, 1.28050608e-02,\n",
       "       9.99587137e-01, 9.99682676e-01, 1.71694989e-04, 7.54570009e-03,\n",
       "       9.99704339e-01, 2.38448395e-04, 9.97425047e-01, 2.53011405e-02,\n",
       "       9.99713909e-01, 9.38118387e-01, 2.52645343e-04, 9.99531073e-01,\n",
       "       4.95142955e-04, 1.01511842e-02, 9.99737920e-01, 9.57664926e-01,\n",
       "       1.77741581e-02, 1.14730664e-02, 1.98874198e-04, 1.87904725e-04,\n",
       "       9.96498014e-01, 4.54494571e-05, 9.23338624e-05, 4.06829828e-04,\n",
       "       5.90616066e-04, 9.98352908e-01, 9.85935394e-04, 9.95798156e-01,\n",
       "       9.94403835e-01, 2.54386877e-01, 1.22027934e-03, 2.13478221e-04,\n",
       "       1.01931080e-04, 9.20353104e-01, 9.97719506e-01, 2.51329679e-01,\n",
       "       9.99837454e-01, 9.99686699e-01, 9.95893821e-01, 9.98398272e-01,\n",
       "       9.97036643e-01, 7.94122351e-04, 9.99306780e-01, 1.64479398e-03,\n",
       "       9.97925903e-01, 8.37048134e-01, 5.75956557e-03, 9.96630552e-01,\n",
       "       9.99920532e-01, 1.29231040e-04, 4.14318531e-03, 9.32726719e-01,\n",
       "       1.11833349e-02, 9.99097140e-01, 9.80813386e-04, 9.98668057e-01,\n",
       "       9.96715283e-01, 5.68901546e-01, 5.09120585e-03, 6.41003009e-04,\n",
       "       9.74447564e-01, 9.97803665e-01, 6.73800606e-04, 2.43905365e-04,\n",
       "       3.20387973e-03, 9.98072661e-01, 9.99649408e-01, 9.99883578e-01,\n",
       "       9.98010914e-01, 9.99142416e-01, 1.18805775e-04, 9.99422185e-01,\n",
       "       3.61781535e-04, 1.43913079e-03, 2.02335500e-02, 9.98145035e-01,\n",
       "       2.28246792e-04, 9.94812080e-01, 1.38894223e-04, 4.41100547e-04,\n",
       "       2.52974846e-01, 9.99572247e-01, 8.05565819e-04, 9.99710327e-01,\n",
       "       9.09603115e-01, 4.00000000e-01, 9.99013610e-01, 1.30134445e-03,\n",
       "       9.99950826e-01, 9.99624570e-01, 3.21466237e-03, 1.98103130e-03,\n",
       "       3.61547255e-04, 7.04188390e-04, 3.54045361e-03, 8.74610838e-01,\n",
       "       8.55016953e-01, 9.93914625e-01, 9.99937986e-01, 9.99788265e-01,\n",
       "       4.12898814e-04, 1.90936270e-04, 6.50046961e-01, 9.97616338e-01,\n",
       "       8.66470910e-05, 4.73639341e-04, 9.99676002e-01, 4.79869540e-02,\n",
       "       5.85786059e-05, 1.01715621e-03, 9.99791959e-01, 2.32547248e-04,\n",
       "       9.97846793e-01, 4.67182649e-04, 9.98577883e-01, 9.89623169e-01,\n",
       "       3.22722905e-03, 7.19891147e-03, 9.98322938e-01, 1.74690140e-04,\n",
       "       6.40442609e-03, 9.81582371e-01, 1.00578635e-03, 9.99654027e-01,\n",
       "       9.99700809e-01, 2.16468905e-02, 9.73786408e-01, 9.98997991e-01,\n",
       "       9.99492634e-01, 9.98487269e-01, 2.61999677e-02, 9.26859322e-03,\n",
       "       1.88139840e-04, 2.16625354e-01, 9.98664125e-01, 9.94631287e-01,\n",
       "       9.99497458e-01, 3.32095395e-03, 8.65847426e-01, 3.93662823e-02,\n",
       "       9.97414085e-01, 5.98726645e-04, 3.20423905e-04, 2.08881234e-02,\n",
       "       9.99753506e-01, 9.99699696e-01, 9.93765965e-01, 3.79310863e-01,\n",
       "       9.99723032e-01, 9.99801101e-01, 9.99531973e-01, 5.24791491e-02,\n",
       "       9.98288868e-01, 9.81190446e-04, 8.95960714e-03, 9.99743739e-01,\n",
       "       4.13197691e-02, 9.95692180e-01, 2.41601897e-03, 1.68765500e-04,\n",
       "       9.65912717e-01, 7.13813604e-01, 2.40871252e-03, 3.09144647e-04,\n",
       "       5.73120855e-04, 9.99846459e-01, 9.98726322e-01, 3.29924811e-04,\n",
       "       8.33816561e-01, 9.99433080e-01, 9.99249674e-01, 9.08686177e-02,\n",
       "       9.98808096e-01, 6.15110676e-04, 2.34606140e-04, 3.40527686e-01,\n",
       "       9.84211561e-01, 7.73824920e-03, 1.01161612e-03, 9.97982004e-01,\n",
       "       9.83571219e-01, 1.14738116e-03, 4.29640898e-03, 4.01473126e-03,\n",
       "       9.99180031e-01, 1.56827748e-03, 8.64695922e-01, 9.98657524e-01,\n",
       "       9.98151816e-01, 9.99812608e-01, 9.87821767e-05, 9.99235803e-01,\n",
       "       9.81765813e-01, 3.17280390e-02, 5.62216819e-03, 9.79781302e-01,\n",
       "       9.36845575e-03, 4.17955191e-02, 1.89923767e-04, 9.61513308e-01,\n",
       "       7.64299742e-04, 7.82767358e-04, 5.28047094e-01, 9.98987978e-01,\n",
       "       2.16740794e-01, 3.73648948e-04, 4.12219256e-04, 8.82183276e-05,\n",
       "       9.99488042e-01, 9.99898260e-01, 9.98857924e-01, 9.08425649e-04,\n",
       "       9.94440452e-01, 1.12560664e-03, 9.98896946e-01, 9.99722057e-01,\n",
       "       9.05521466e-04, 1.54200786e-02, 9.75682020e-01, 9.15277600e-04,\n",
       "       6.95287989e-04, 8.51555060e-01, 1.56160089e-04, 9.97508059e-01,\n",
       "       9.97738958e-01, 9.98940725e-01, 9.98926181e-01, 9.92828209e-01,\n",
       "       7.75267965e-03, 7.14777417e-03, 7.95505268e-03, 9.99668460e-01,\n",
       "       9.99579709e-01, 9.97131533e-01, 9.81337870e-01, 1.63295786e-04,\n",
       "       3.65643892e-02, 1.49986668e-04, 8.29589226e-04, 9.98205542e-01,\n",
       "       6.09050788e-04, 9.98949999e-01, 4.35228921e-02, 9.93903619e-01,\n",
       "       1.42936924e-02, 1.74546150e-04, 9.99648359e-01, 1.85584203e-04,\n",
       "       3.06889571e-04, 9.95478946e-01, 9.00793955e-02, 2.82535150e-03,\n",
       "       9.91575517e-01, 9.97882672e-01, 9.95215123e-01, 3.97974721e-04,\n",
       "       9.99529611e-01, 3.61466410e-04, 9.80921395e-01, 3.25700259e-04,\n",
       "       9.64631039e-04, 9.99189856e-01, 9.99298221e-01, 9.99781625e-01,\n",
       "       9.99683711e-01, 1.10019827e-03, 1.48797750e-03, 9.98575516e-01,\n",
       "       9.97954762e-01, 9.80923191e-01, 2.45469745e-03, 1.08871722e-03,\n",
       "       9.99737466e-01, 9.64294631e-01, 3.71807247e-03, 9.63460555e-01,\n",
       "       9.99530568e-01, 1.18363373e-04, 3.25225473e-04, 5.59918953e-01,\n",
       "       9.70990798e-01, 1.15536398e-03, 3.00725647e-04, 9.97654918e-01,\n",
       "       8.65860063e-01, 9.75764359e-01, 9.94779935e-01, 2.33534476e-03,\n",
       "       9.93073964e-01, 9.99470567e-01, 1.84367336e-04, 3.62878544e-04,\n",
       "       9.58218748e-02, 1.41602947e-04, 2.51875168e-03, 9.90820179e-01,\n",
       "       9.26317938e-02, 9.99843440e-01, 9.18963563e-03, 9.99571811e-01,\n",
       "       9.95534730e-01, 8.12192374e-04, 1.25862647e-03, 1.97972112e-03,\n",
       "       3.51152033e-03, 1.41765212e-03, 2.67861375e-04, 1.55173561e-03,\n",
       "       9.89834927e-02, 1.19267678e-04, 6.08181702e-01, 9.98761882e-01,\n",
       "       9.53051334e-01, 4.16198823e-04, 7.13850251e-03, 9.95302514e-01,\n",
       "       9.48778001e-01, 9.99104985e-01, 5.63222711e-04, 1.49482638e-03,\n",
       "       9.26899922e-01, 1.12214093e-01, 9.40175747e-01, 9.49471219e-01,\n",
       "       8.89480415e-04, 9.84780996e-01, 1.23678768e-04, 9.92821849e-01,\n",
       "       5.51842422e-06, 9.99856419e-01, 4.77895223e-04, 1.89913052e-02,\n",
       "       4.64457289e-03, 3.08524215e-05, 9.98412827e-01, 9.99471079e-01,\n",
       "       9.98811680e-01, 9.80195841e-01, 9.97959924e-01, 6.51132108e-04,\n",
       "       5.62635060e-04, 1.27302884e-03, 9.41618197e-01, 1.40273372e-02,\n",
       "       9.97995806e-01, 9.99683578e-01, 2.42298863e-03, 9.99786831e-01,\n",
       "       9.92389304e-01, 9.90080894e-01, 9.96736116e-01, 1.65699467e-04,\n",
       "       1.92266606e-04, 9.99181366e-01, 1.33066780e-03, 3.59520936e-04,\n",
       "       2.29721754e-03, 8.68480746e-04, 9.89865555e-01, 9.99592163e-01,\n",
       "       5.07675512e-04, 9.99614271e-01, 9.99730893e-01, 1.63454041e-01,\n",
       "       9.98058034e-01, 3.91378646e-01, 8.24767204e-01, 3.99225830e-04,\n",
       "       2.87640729e-05, 1.50845642e-04, 2.08327713e-03, 2.38884906e-02,\n",
       "       4.29967780e-01, 1.99306185e-04, 4.19113174e-04, 5.24225488e-04,\n",
       "       9.99588616e-01, 9.98364362e-01, 9.99525235e-01, 2.41324482e-02,\n",
       "       7.91720015e-02, 1.20066162e-03, 1.14161744e-03, 9.86114285e-01,\n",
       "       8.74471923e-05, 9.97992739e-01, 9.95464239e-01, 1.46813084e-05,\n",
       "       7.38617155e-01, 9.93158475e-01, 3.06468336e-04, 9.96058672e-01,\n",
       "       2.53845910e-04, 7.92721483e-04, 4.08260557e-04, 7.24604694e-03,\n",
       "       9.95156736e-01, 9.91902006e-01, 1.42155553e-02, 9.94139079e-01,\n",
       "       9.99670255e-01, 1.45974675e-04, 9.98929250e-01, 1.01287604e-01,\n",
       "       3.31324046e-04, 6.02508161e-04, 6.81200136e-04, 9.82800028e-01,\n",
       "       5.05269706e-03, 2.55806789e-03, 8.64821650e-03, 2.37022455e-02,\n",
       "       9.98118869e-01, 9.99349849e-01, 9.99036429e-01, 1.32069823e-04,\n",
       "       1.20770575e-04, 2.81368369e-04, 8.20664656e-05, 9.99024775e-01,\n",
       "       8.94144511e-01, 1.11925895e-03, 9.99648104e-01, 9.99737535e-01,\n",
       "       7.16598298e-04, 9.99648236e-01, 9.99937098e-01, 9.99058191e-01,\n",
       "       2.26525392e-03, 9.99688267e-01, 1.04676521e-02, 1.07512260e-01,\n",
       "       3.75214996e-01, 1.95211090e-04, 1.12246540e-04, 9.99473799e-01,\n",
       "       4.27038539e-03, 9.99047151e-01, 3.38628900e-04, 3.76406466e-03,\n",
       "       2.48915845e-03, 9.98950065e-01, 2.71351065e-04, 2.17461214e-04,\n",
       "       3.45868263e-03, 3.49943969e-04, 7.81482451e-05, 9.99740967e-01,\n",
       "       9.98171229e-01, 9.99773425e-01, 9.99266092e-01, 4.17291943e-02,\n",
       "       1.46776842e-03, 6.83287884e-04, 9.99318872e-01, 1.79141299e-03,\n",
       "       4.10444428e-03, 9.99876757e-01, 1.93333180e-03, 6.25872311e-04,\n",
       "       9.99771022e-01, 9.97632794e-01, 1.14480871e-03, 1.54707572e-03,\n",
       "       3.00767504e-03, 9.99265414e-01, 4.72611544e-03, 9.97474508e-01,\n",
       "       5.32437654e-02, 9.93454564e-01, 3.03185795e-02, 9.98155901e-01,\n",
       "       9.99237149e-01, 3.14437712e-04, 9.97636733e-01, 5.14719782e-03,\n",
       "       9.98648718e-01, 9.99674206e-01, 9.99641223e-01, 1.02411042e-04,\n",
       "       9.93979144e-01, 9.94281952e-01, 1.77702224e-04, 2.32538559e-03,\n",
       "       6.78063498e-03, 5.83282526e-04, 9.95440293e-01, 1.41384944e-03,\n",
       "       9.96756818e-01, 1.06907020e-03, 2.94098998e-03, 9.97334382e-01,\n",
       "       1.04197748e-01, 1.75440123e-03, 9.94423152e-01, 9.93497602e-01,\n",
       "       1.01343199e-02, 4.91076598e-04, 9.80177869e-01, 9.99821107e-01,\n",
       "       1.66019532e-02, 5.35721579e-04, 9.99430862e-01, 1.67055537e-03,\n",
       "       3.90781513e-04, 2.75240692e-04, 3.73325380e-02, 9.99602933e-01,\n",
       "       9.95384742e-01, 9.99769543e-01, 9.99158108e-01, 9.99489692e-01,\n",
       "       9.99631372e-01, 9.98646461e-01, 1.49996658e-04, 2.94459254e-02,\n",
       "       9.98694535e-01, 3.68839441e-03, 9.99303615e-01, 9.98849209e-01,\n",
       "       9.97779541e-01, 9.51219147e-01, 1.08870845e-04, 8.95658928e-03,\n",
       "       2.38667483e-03, 9.99683105e-01, 9.96553302e-01, 9.73854729e-01,\n",
       "       3.68006941e-04, 5.81123964e-04, 3.63475512e-04, 8.66847836e-04,\n",
       "       9.99945050e-01, 7.10062606e-04, 9.99430379e-01, 2.55804145e-04,\n",
       "       9.99139131e-01, 9.36314844e-04, 4.01262108e-03, 8.79121570e-01,\n",
       "       3.08357148e-04, 9.99717791e-01, 6.70546360e-04, 9.86175341e-01,\n",
       "       9.99814280e-01, 9.16613332e-04, 9.98047581e-01, 9.82278093e-01,\n",
       "       8.43506662e-01, 1.79314160e-02, 9.92942180e-01, 5.36263604e-03,\n",
       "       9.99658034e-01, 9.98770650e-01, 6.17785843e-03, 9.99559504e-01,\n",
       "       1.65284892e-04, 7.11188277e-01, 1.07494991e-04, 9.02183482e-04,\n",
       "       7.86111375e-01, 9.99810426e-01, 4.47618089e-05, 9.99816953e-01,\n",
       "       1.04549977e-04, 9.98892728e-01, 9.99701449e-01, 3.40487271e-04,\n",
       "       5.88619130e-04, 7.94522255e-02, 8.44721761e-03, 1.44433437e-03,\n",
       "       3.00767784e-04, 9.08957691e-01, 1.81482608e-04, 2.37960954e-04,\n",
       "       6.86670216e-04, 1.19060375e-03, 3.63735778e-04, 7.59688473e-05,\n",
       "       9.96918138e-01, 1.80969043e-05, 9.99814160e-01, 1.06382970e-03,\n",
       "       9.91647292e-01, 9.61915269e-01, 1.88335553e-04, 9.95577318e-01,\n",
       "       4.77571551e-03, 9.99260327e-01, 7.62983365e-04, 1.65797012e-02,\n",
       "       5.86551448e-05, 9.62174548e-01, 9.99862337e-01, 9.61757391e-01,\n",
       "       6.88067282e-03, 3.33053101e-02, 1.20019772e-04, 9.98491897e-01,\n",
       "       1.96180711e-01, 9.99683110e-01, 9.98531640e-01, 9.78135808e-01,\n",
       "       3.76771743e-05, 9.53285878e-05, 5.17817904e-05, 9.97765328e-01,\n",
       "       9.60627431e-01, 5.81401985e-01, 9.99752383e-01, 9.85160669e-01,\n",
       "       4.29062241e-03, 2.21493093e-04, 9.98842788e-01, 3.63213808e-04,\n",
       "       3.37331399e-02, 9.99753999e-01, 3.51573566e-04, 9.99318692e-01,\n",
       "       1.15207651e-04, 7.17953894e-01, 9.99911896e-01, 9.88794060e-01,\n",
       "       1.36051025e-04, 3.06769168e-04, 9.03718053e-05, 9.99158405e-01,\n",
       "       9.97406004e-01, 6.86760357e-03, 4.50727342e-04, 9.80949349e-01,\n",
       "       9.94987005e-01, 9.98117648e-01, 7.89600071e-04, 1.14760634e-04,\n",
       "       9.99256259e-01, 1.84876073e-03, 1.01992979e-03, 1.25186080e-03,\n",
       "       9.89079793e-01, 4.21799208e-04, 9.99350597e-01, 3.79847080e-05,\n",
       "       8.16218866e-04, 9.99376821e-01, 9.99094571e-01, 9.98672530e-01,\n",
       "       6.39006603e-04, 9.93377047e-01, 3.16250897e-03, 5.94996361e-03,\n",
       "       2.02455728e-04, 9.51531424e-01, 9.97853510e-01, 3.65695851e-04,\n",
       "       9.96225560e-01, 9.79979149e-01, 9.90545566e-01, 5.86259025e-04,\n",
       "       9.07365691e-03, 9.99280307e-01, 1.48732997e-03, 9.99630924e-01,\n",
       "       1.40464640e-03, 3.89770056e-03, 9.99939820e-01, 2.25009901e-01,\n",
       "       2.59799495e-04, 1.67397651e-03, 1.75444321e-03, 9.06965856e-01,\n",
       "       8.84442859e-01, 9.94468208e-01, 1.06758381e-03, 9.91895231e-01,\n",
       "       9.47824419e-01, 9.99621882e-01, 1.80545306e-03, 6.22157300e-01,\n",
       "       9.97723112e-01, 4.10000000e-01, 8.93214681e-02, 7.08491244e-01,\n",
       "       9.94435735e-01, 1.41843091e-02, 3.92755572e-02, 2.43086820e-04,\n",
       "       1.48863270e-04, 4.38916253e-04, 1.47285870e-03, 2.00956209e-02,\n",
       "       9.97226770e-01, 8.68884773e-02, 4.76127982e-05, 1.12107044e-03,\n",
       "       9.88774464e-01, 2.62365761e-01, 9.99515466e-01, 9.97064431e-01,\n",
       "       9.88144581e-01, 9.97244454e-01, 5.28897469e-04, 1.68985312e-04,\n",
       "       2.77219741e-02, 9.53699338e-05, 9.99879588e-01, 9.98237292e-01,\n",
       "       9.98865638e-01, 9.85329582e-01, 9.99735317e-01, 1.96405677e-04,\n",
       "       9.99667688e-01, 2.89571876e-02, 9.18734963e-01, 7.40207140e-05,\n",
       "       8.94154191e-04, 3.77007923e-04, 1.48616964e-04, 4.46476125e-04,\n",
       "       7.40084325e-04, 7.74229718e-04, 9.99539213e-01, 1.20688459e-03,\n",
       "       9.95585762e-01, 9.95775056e-01, 9.99842612e-01, 1.68772140e-01,\n",
       "       4.57599136e-03, 8.66271160e-01, 6.76926265e-04, 9.99893882e-01,\n",
       "       2.07606360e-04, 9.96222475e-01, 6.91901104e-03, 9.97347087e-01,\n",
       "       4.38040536e-02, 9.99508074e-01, 9.88357752e-01, 9.94619692e-01,\n",
       "       9.56856772e-01, 4.23179348e-04, 9.98218965e-01, 9.72690357e-01,\n",
       "       1.08645869e-03, 9.87623612e-01, 9.99817631e-01, 9.99299200e-01,\n",
       "       6.64130311e-03, 9.99676572e-01, 1.38243592e-04, 9.77510770e-01,\n",
       "       9.93573992e-01, 2.91773491e-03, 9.99690019e-01, 9.99774572e-01,\n",
       "       9.99912998e-01, 2.01760431e-03, 8.74400547e-02, 9.96306939e-01,\n",
       "       1.13983849e-03, 9.99082143e-01, 3.49852964e-04, 1.64099059e-03,\n",
       "       2.15901080e-03, 8.97402345e-01, 7.63484584e-01, 9.99334397e-01,\n",
       "       9.82223344e-01, 9.83431163e-01, 3.71148731e-02, 9.07333913e-01,\n",
       "       2.78423080e-03, 1.70931699e-03, 9.87638660e-01, 9.97405023e-01,\n",
       "       9.99812469e-01, 4.51666754e-03, 1.50963547e-04, 9.98871746e-01,\n",
       "       9.72164460e-03, 9.96337253e-01, 4.38790953e-01, 1.92969149e-03,\n",
       "       9.96633524e-01, 9.96749412e-01, 7.05669456e-04, 1.26714048e-04])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred2_for_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "990892ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_pred2_for_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "210d5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_pred2_for_class1).to_csv(\"PATWRO_digits_prediction.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "15ec0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = column_cases[\"case8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "e1a2be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df = pd.DataFrame(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "c7aaf139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>4779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>4791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>4885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>4917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>4979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       2\n",
       "1      14\n",
       "2      34\n",
       "3      58\n",
       "4      67\n",
       "..    ...\n",
       "175  4779\n",
       "176  4791\n",
       "177  4885\n",
       "178  4917\n",
       "179  4979\n",
       "\n",
       "[180 rows x 1 columns]"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "52fd23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df.to_csv(\"PATWRO_digits_features.txt\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
